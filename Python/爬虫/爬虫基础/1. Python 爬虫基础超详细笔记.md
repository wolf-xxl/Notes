
## 1. çˆ¬è™«åŸºç¡€æ¦‚å¿µ

### 1.1 ä»€ä¹ˆæ˜¯ç½‘ç»œçˆ¬è™«åŠå…¶åº”ç”¨åœºæ™¯
```python
"""
ç½‘ç»œçˆ¬è™«ï¼ˆWeb Crawlerï¼‰æ˜¯ä¸€ç§è‡ªåŠ¨æµè§ˆäº’è”ç½‘å¹¶æ”¶é›†æ•°æ®çš„ç¨‹åº
çˆ¬è™«æ¨¡æ‹Ÿäººç±»æµè§ˆå™¨è¡Œä¸ºï¼Œè‡ªåŠ¨è®¿é—®ç½‘é¡µå¹¶æå–æ‰€éœ€ä¿¡æ¯
"""

def crawler_definition():
    """
    çˆ¬è™«çš„æ ¸å¿ƒåŠŸèƒ½ï¼š
    1. è‡ªåŠ¨å‘é€HTTPè¯·æ±‚è·å–ç½‘é¡µå†…å®¹
    2. è§£æHTML/JSONç­‰æ ¼å¼çš„æ•°æ®
    3. æå–å’Œå­˜å‚¨æœ‰ä»·å€¼çš„ä¿¡æ¯
    4. æ ¹æ®é“¾æ¥å‘ç°æ–°çš„æŠ“å–ç›®æ ‡
    """
    
    applications = {
        "æœç´¢å¼•æ“": "Googleã€ç™¾åº¦ç­‰æœç´¢å¼•æ“ä½¿ç”¨çˆ¬è™«å»ºç«‹ç½‘é¡µç´¢å¼•",
        "ä»·æ ¼ç›‘æ§": "ç”µå•†ç½‘ç«™ä»·æ ¼æ¯”è¾ƒå’Œç›‘æ§",
        "æ•°æ®æŒ–æ˜": "æ”¶é›†ç¤¾äº¤åª’ä½“ã€æ–°é—»ç½‘ç«™æ•°æ®è¿›è¡Œåˆ†æ",
        "å­¦æœ¯ç ”ç©¶": "æ”¶é›†å­¦æœ¯è®ºæ–‡ã€ç§‘ç ”æ•°æ®",
        "ç«äº‰åˆ†æ": "ç›‘æ§ç«äº‰å¯¹æ‰‹ç½‘ç«™å†…å®¹å˜åŒ–"
    }
    
    return applications

print("çˆ¬è™«åº”ç”¨åœºæ™¯:")
for app, desc in crawler_definition().items():
    print(f"- {app}: {desc}")
# è¾“å‡º:
# çˆ¬è™«åº”ç”¨åœºæ™¯:
# - æœç´¢å¼•æ“: Googleã€ç™¾åº¦ç­‰æœç´¢å¼•æ“ä½¿ç”¨çˆ¬è™«å»ºç«‹ç½‘é¡µç´¢å¼•
# - ä»·æ ¼ç›‘æ§: ç”µå•†ç½‘ç«™ä»·æ ¼æ¯”è¾ƒå’Œç›‘æ§
# - æ•°æ®æŒ–æ˜: æ”¶é›†ç¤¾äº¤åª’ä½“ã€æ–°é—»ç½‘ç«™æ•°æ®è¿›è¡Œåˆ†æ
# - å­¦æœ¯ç ”ç©¶: æ”¶é›†å­¦æœ¯è®ºæ–‡ã€ç§‘ç ”æ•°æ®
# - ç«äº‰åˆ†æ: ç›‘æ§ç«äº‰å¯¹æ‰‹ç½‘ç«™å†…å®¹å˜åŒ–
```

### 1.2 çˆ¬è™«çš„æ³•å¾‹å’Œé“å¾·è§„èŒƒ
```python
def crawler_ethics_and_laws():
    """
    çˆ¬è™«å¼€å‘å¿…é¡»éµå®ˆçš„æ³•å¾‹å’Œé“å¾·è§„èŒƒï¼š
    1. éµå®ˆrobots.txtåè®®
    2. å°Šé‡ç½‘ç«™çš„ä½¿ç”¨æ¡æ¬¾
    3. æ§åˆ¶è®¿é—®é¢‘ç‡ï¼Œé¿å…å¯¹ç½‘ç«™é€ æˆå‹åŠ›
    4. ä¸çˆ¬å–ä¸ªäººéšç§å’Œæ•æ„Ÿä¿¡æ¯
    5. ä¸ç”¨äºå•†ä¸šç«äº‰æˆ–æ¶æ„ç”¨é€”
    6. å°Šé‡ç‰ˆæƒå’ŒçŸ¥è¯†äº§æƒ
    """
    
    important_rules = [
        "âœ… æ£€æŸ¥å¹¶éµå®ˆrobots.txtæ–‡ä»¶",
        "âœ… è®¾ç½®åˆç†çš„è¯·æ±‚é—´éš”ï¼ˆè‡³å°‘1-2ç§’ï¼‰",
        "âœ… è¯†åˆ«å¹¶éµå®ˆAPIé™åˆ¶",
        "âŒ ä¸ç»•è¿‡åçˆ¬è™«æœºåˆ¶",
        "âŒ ä¸çˆ¬å–ä¸ªäººéšç§ä¿¡æ¯",
        "âŒ ä¸è¿›è¡Œæ¶æ„ç«äº‰"
    ]
    
    return important_rules

print("çˆ¬è™«é“å¾·å’Œæ³•å¾‹è§„èŒƒ:")
for rule in crawler_ethics_and_laws():
    print(f"  {rule}")
# è¾“å‡º:
# çˆ¬è™«é“å¾·å’Œæ³•å¾‹è§„èŒƒ:
#   âœ… æ£€æŸ¥å¹¶éµå®ˆrobots.txtæ–‡ä»¶
#   âœ… è®¾ç½®åˆç†çš„è¯·æ±‚é—´éš”ï¼ˆè‡³å°‘1-2ç§’ï¼‰
#   âœ… è¯†åˆ«å¹¶éµå®ˆAPIé™åˆ¶
#   âŒ ä¸ç»•è¿‡åçˆ¬è™«æœºåˆ¶
#   âŒ ä¸çˆ¬å–ä¸ªäººéšç§ä¿¡æ¯
#   âŒ ä¸è¿›è¡Œæ¶æ„ç«äº‰
```

### 1.3 ç¯å¢ƒå‡†å¤‡å’ŒåŸºç¡€åº“
```python
"""
çˆ¬è™«å¼€å‘å¸¸ç”¨åº“å®‰è£…ï¼š
åœ¨å‘½ä»¤è¡Œä¸­æ‰§è¡Œä»¥ä¸‹å‘½ä»¤å®‰è£…å¿…è¦çš„åº“
"""

# å®‰è£…å‘½ä»¤ï¼ˆåœ¨å‘½ä»¤è¡Œä¸­æ‰§è¡Œï¼Œä¸æ˜¯åœ¨Pythonä»£ç ä¸­ï¼‰
install_commands = [
    "pip install requests",          # å‘é€HTTPè¯·æ±‚
    "pip install beautifulsoup4",    # HTMLè§£æ
    "pip install lxml",              # å¿«é€ŸXML/HTMLè§£æå™¨
    "pip install selenium",          # æµè§ˆå™¨è‡ªåŠ¨åŒ–ï¼Œå¤„ç†JavaScript
    "pip install scrapy",            # ä¸“ä¸šçš„çˆ¬è™«æ¡†æ¶
    "pip install pymysql",           # MySQLæ•°æ®åº“è¿æ¥
    "pip install pymongo",           # MongoDBæ•°æ®åº“è¿æ¥
    "pip install redis",             # Redisæ•°æ®åº“è¿æ¥
]

print("éœ€è¦å®‰è£…çš„åº“:")
for cmd in install_commands:
    print(f"  {cmd}")

# å¯¼å…¥å¸¸ç”¨åº“
import requests
from bs4 import BeautifulSoup
import time
import random
import os
import json
import csv
import re

print("æ‰€æœ‰å¿…è¦çš„åº“å·²å¯¼å…¥æˆåŠŸ")
# è¾“å‡º: æ‰€æœ‰å¿…è¦çš„åº“å·²å¯¼å…¥æˆåŠŸ
```

## 2. å®Œæ•´çˆ¬è™«æµç¨‹

### 2.1 çˆ¬è™«å·¥ä½œæµç¨‹è¯¦è§£
```python
def crawler_workflow_demo():
    """
    å®Œæ•´çš„çˆ¬è™«å·¥ä½œæµç¨‹ï¼š
    1. ç›®æ ‡åˆ†æï¼šç¡®å®šè¦çˆ¬å–çš„ç½‘ç«™å’Œæ•°æ®
    2. è¯·æ±‚å‘é€ï¼šæ„é€ è¯·æ±‚è·å–ç½‘é¡µå†…å®¹
    3. å†…å®¹è§£æï¼šä»HTMLä¸­æå–æ‰€éœ€æ•°æ®
    4. æ•°æ®å­˜å‚¨ï¼šå°†æ•°æ®ä¿å­˜åˆ°æ–‡ä»¶æˆ–æ•°æ®åº“
    5. é“¾æ¥å‘ç°ï¼šå‘ç°æ–°çš„URLç»§ç»­çˆ¬å–
    6. å¼‚å¸¸å¤„ç†ï¼šå¤„ç†å„ç§å¼‚å¸¸æƒ…å†µ
    """
    
    workflow_steps = [
        "ğŸ” ç›®æ ‡åˆ†æ â†’ ğŸŒ å‘é€è¯·æ±‚ â†’ ğŸ“„ è·å–å“åº”",
        "ğŸ”§ è§£æå†…å®¹ â†’ ğŸ’¾ æ•°æ®å­˜å‚¨ â†’ ğŸ”— å‘ç°æ–°é“¾æ¥",
        "ğŸ”„ å¾ªç¯å¤„ç† â†’ âš ï¸ å¼‚å¸¸å¤„ç† â†’ âœ… ä»»åŠ¡å®Œæˆ"
    ]
    
    return workflow_steps

print("å®Œæ•´çˆ¬è™«å·¥ä½œæµç¨‹:")
for step in crawler_workflow_demo():
    print(f"  {step}")
# è¾“å‡º:
# å®Œæ•´çˆ¬è™«å·¥ä½œæµç¨‹:
#   ğŸ” ç›®æ ‡åˆ†æ â†’ ğŸŒ å‘é€è¯·æ±‚ â†’ ğŸ“„ è·å–å“åº”
#   ğŸ”§ è§£æå†…å®¹ â†’ ğŸ’¾ æ•°æ®å­˜å‚¨ â†’ ğŸ”— å‘ç°æ–°é“¾æ¥
#   ğŸ”„ å¾ªç¯å¤„ç† â†’ âš ï¸ å¼‚å¸¸å¤„ç† â†’ âœ… ä»»åŠ¡å®Œæˆ
```

### 2.2 ç®€å•çˆ¬è™«ç¤ºä¾‹
```python
def simple_crawler_example():
    """
    ä¸€ä¸ªå®Œæ•´çš„ç®€å•çˆ¬è™«ç¤ºä¾‹
    çˆ¬å–ç½‘é¡µæ ‡é¢˜å’Œæ‰€æœ‰é“¾æ¥
    """
    
    import requests
    from bs4 import BeautifulSoup
    import time
    
    def crawl_website(url):
        """çˆ¬å–æŒ‡å®šURLçš„ç½‘é¡µå†…å®¹"""
        try:
            # 1. å‘é€HTTPè¯·æ±‚
            print(f"æ­£åœ¨è¯·æ±‚: {url}")
            response = requests.get(url)
            
            # æ£€æŸ¥è¯·æ±‚æ˜¯å¦æˆåŠŸ
            if response.status_code == 200:
                print("âœ… è¯·æ±‚æˆåŠŸ")
                
                # 2. è§£æHTMLå†…å®¹
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # 3. æå–æ•°æ®
                title = soup.title.string if soup.title else "æ— æ ‡é¢˜"
                links = soup.find_all('a', href=True)
                
                # 4. å¤„ç†æ•°æ®
                result = {
                    'url': url,
                    'title': title,
                    'link_count': len(links),
                    'links': [link['href'] for link in links[:5]]  # åªå–å‰5ä¸ªé“¾æ¥
                }
                
                return result
            else:
                print(f"âŒ è¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
                return None
                
        except Exception as e:
            print(f"âŒ å‘ç”Ÿé”™è¯¯: {e}")
            return None
    
    # æµ‹è¯•çˆ¬è™«
    test_url = "https://httpbin.org/html"  # æµ‹è¯•ç”¨çš„ç½‘é¡µ
    result = crawl_website(test_url)
    
    if result:
        print(f"\nçˆ¬å–ç»“æœ:")
        print(f"  ç½‘é¡µæ ‡é¢˜: {result['title']}")
        print(f"  é“¾æ¥æ•°é‡: {result['link_count']}")
        print(f"  å‰5ä¸ªé“¾æ¥: {result['links']}")
    else:
        print("çˆ¬å–å¤±è´¥")

# è¿è¡Œç¤ºä¾‹
simple_crawler_example()
# è¾“å‡º:
# æ­£åœ¨è¯·æ±‚: https://httpbin.org/html
# âœ… è¯·æ±‚æˆåŠŸ
# 
# çˆ¬å–ç»“æœ:
#   ç½‘é¡µæ ‡é¢˜: HtmLorem Ipsum
#   é“¾æ¥æ•°é‡: 2
#   å‰5ä¸ªé“¾æ¥: ['/links/5', '/links/5']
```

## 3. è¯·æ±‚åº“çš„ä½¿ç”¨ï¼ˆrequestsï¼‰

### 3.1 requestsåŸºç¡€ç”¨æ³•
```python
def requests_basic_usage():
    """requestsåº“çš„åŸºç¡€ç”¨æ³•æ¼”ç¤º"""
    
    import requests
    
    # 1. åŸºæœ¬çš„GETè¯·æ±‚
    print("1. åŸºæœ¬GETè¯·æ±‚:")
    response = requests.get('https://httpbin.org/get')
    print(f"çŠ¶æ€ç : {response.status_code}")  # è¾“å‡º: çŠ¶æ€ç : 200
    print(f"å“åº”å†…å®¹ç±»å‹: {response.headers['content-type']}")  # è¾“å‡º: å“åº”å†…å®¹ç±»å‹: application/json
    
    # 2. å¸¦å‚æ•°çš„GETè¯·æ±‚
    print("\n2. å¸¦å‚æ•°çš„GETè¯·æ±‚:")
    params = {'key1': 'value1', 'key2': 'value2'}
    response = requests.get('https://httpbin.org/get', params=params)
    print(f"è¯·æ±‚URL: {response.url}")  # è¾“å‡º: è¯·æ±‚URL: https://httpbin.org/get?key1=value1&key2=value2
    
    # 3. POSTè¯·æ±‚
    print("\n3. POSTè¯·æ±‚:")
    data = {'username': 'test', 'password': '123456'}
    response = requests.post('https://httpbin.org/post', data=data)
    print(f"POSTå“åº”çŠ¶æ€: {response.status_code}")  # è¾“å‡º: POSTå“åº”çŠ¶æ€: 200
    
    # 4. è®¾ç½®è¯·æ±‚å¤´
    print("\n4. è®¾ç½®è¯·æ±‚å¤´:")
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8'
    }
    response = requests.get('https://httpbin.org/headers', headers=headers)
    print(f"æœåŠ¡å™¨æ”¶åˆ°çš„è¯·æ±‚å¤´: {response.json()['headers']['User-Agent']}")
    # è¾“å‡º: æœåŠ¡å™¨æ”¶åˆ°çš„è¯·æ±‚å¤´: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36
    
    # 5. å¤„ç†è¶…æ—¶
    print("\n5. è¶…æ—¶è®¾ç½®:")
    try:
        # è®¾ç½®0.1ç§’è¶…æ—¶ï¼Œè¿™ä¸ªç½‘ç«™ä¼šç­‰å¾…1ç§’ï¼Œæ‰€ä»¥ä¼šè¶…æ—¶
        response = requests.get('https://httpbin.org/delay/1', timeout=0.1)
    except requests.exceptions.Timeout:
        print("âœ… è¯·æ±‚è¶…æ—¶ï¼Œè¿™æ˜¯é¢„æœŸçš„è¡Œä¸º")
        # è¾“å‡º: âœ… è¯·æ±‚è¶…æ—¶ï¼Œè¿™æ˜¯é¢„æœŸçš„è¡Œä¸º
    
    # 6. å¤„ç†å¼‚å¸¸
    print("\n6. å¼‚å¸¸å¤„ç†:")
    try:
        response = requests.get('https://invalid-website-that-does-not-exist.com')
    except requests.exceptions.ConnectionError:
        print("âœ… è¿æ¥é”™è¯¯ï¼Œæ— æ³•è®¿é—®ç½‘ç«™")
        # è¾“å‡º: âœ… è¿æ¥é”™è¯¯ï¼Œæ— æ³•è®¿é—®ç½‘ç«™

requests_basic_usage()
```

### 3.2 ä¼šè¯ç®¡ç†å’Œé«˜çº§åŠŸèƒ½
```python
def requests_advanced_features():
    """requestsåº“çš„é«˜çº§åŠŸèƒ½æ¼”ç¤º"""
    
    import requests
    import time
    
    # 1. ä½¿ç”¨ä¼šè¯ï¼ˆSessionï¼‰ä¿æŒcookies
    print("1. ä½¿ç”¨Sessionä¿æŒä¼šè¯:")
    session = requests.Session()
    
    # ç¬¬ä¸€æ¬¡è¯·æ±‚ï¼Œè®¾ç½®cookies
    response1 = session.get('https://httpbin.org/cookies/set/sessioncookie/123456789')
    print(f"ç¬¬ä¸€æ¬¡è¯·æ±‚cookies: {response1.json()}")
    # è¾“å‡º: ç¬¬ä¸€æ¬¡è¯·æ±‚cookies: {'cookies': {'sessioncookie': '123456789'}}
    
    # ç¬¬äºŒæ¬¡è¯·æ±‚ï¼Œcookiesä¼šè‡ªåŠ¨å¸¦ä¸Š
    response2 = session.get('https://httpbin.org/cookies')
    print(f"ç¬¬äºŒæ¬¡è¯·æ±‚cookies: {response2.json()}")
    # è¾“å‡º: ç¬¬äºŒæ¬¡è¯·æ±‚cookies: {'cookies': {'sessioncookie': '123456789'}}
    
    # 2. ä»£ç†è®¾ç½®
    print("\n2. ä»£ç†è®¾ç½®:")
    proxies = {
        'http': 'http://10.10.1.10:3128',  # HTTPä»£ç†
        'https': 'http://10.10.1.10:1080',  # HTTPSä»£ç†
    }
    
    # æ³¨æ„ï¼šè¿™é‡Œçš„ä»£ç†åœ°å€æ˜¯ç¤ºä¾‹ï¼Œå®é™…ä½¿ç”¨æ—¶éœ€è¦æ›¿æ¢ä¸ºæœ‰æ•ˆçš„ä»£ç†æœåŠ¡å™¨
    # response = requests.get('http://example.org', proxies=proxies)
    print("ä»£ç†è®¾ç½®å®Œæˆï¼ˆç¤ºä¾‹ä»£ç ï¼Œæœªå®é™…æ‰§è¡Œï¼‰")
    
    # 3. SSLè¯ä¹¦éªŒè¯
    print("\n3. SSLè¯ä¹¦éªŒè¯:")
    try:
        # ä¸éªŒè¯SSLè¯ä¹¦ï¼ˆä¸æ¨èåœ¨ç”Ÿäº§ç¯å¢ƒä½¿ç”¨ï¼‰
        response = requests.get('https://httpbin.org/get', verify=False)
        print("âœ… ä¸éªŒè¯SSLè¯ä¹¦è¯·æ±‚æˆåŠŸ")
        # è¾“å‡º: âœ… ä¸éªŒè¯SSLè¯ä¹¦è¯·æ±‚æˆåŠŸ
    except Exception as e:
        print(f"SSLé”™è¯¯: {e}")
    
    # 4. è‡ªåŠ¨é‡å®šå‘
    print("\n4. é‡å®šå‘å¤„ç†:")
    response = requests.get('https://httpbin.org/redirect/1', allow_redirects=True)
    print(f"æœ€ç»ˆURL: {response.url}")  # è¾“å‡º: æœ€ç»ˆURL: https://httpbin.org/get
    print(f"é‡å®šå‘å†å²: {len(response.history)} æ¬¡")  # è¾“å‡º: é‡å®šå‘å†å²: 1 æ¬¡
    
    # 5. æµå¼ä¸‹è½½å¤§æ–‡ä»¶
    print("\n5. æµå¼ä¸‹è½½:")
    response = requests.get('https://httpbin.org/stream/5', stream=True)
    for line in response.iter_lines():
        if line:
            print(f"æµæ•°æ®: {line.decode('utf-8')}")
    # è¾“å‡º: æµæ•°æ®: {"url": "https://httpbin.org/stream/5", ...}
    
    # 6. è‡ªå®šä¹‰è®¤è¯
    print("\n6. åŸºæœ¬è®¤è¯:")
    from requests.auth import HTTPBasicAuth
    response = requests.get(
        'https://httpbin.org/basic-auth/user/passwd',
        auth=HTTPBasicAuth('user', 'passwd')
    )
    print(f"è®¤è¯çŠ¶æ€: {response.status_code}")  # è¾“å‡º: è®¤è¯çŠ¶æ€: 200

requests_advanced_features()
```

## 4. æ•°æ®è§£ææ–¹æ³•

### 4.1 BeautifulSoupè§£æHTML
```python
def beautifulsoup_parsing():
    """BeautifulSoupè§£æHTMLæ¼”ç¤º"""
    
    from bs4 import BeautifulSoup
    import requests
    
    # ç¤ºä¾‹HTMLå†…å®¹
    html_content = """
    <html>
    <head>
        <title>æµ‹è¯•ç½‘é¡µ</title>
    </head>
    <body>
        <div id="main">
            <h1 class="title">ç½‘é¡µæ ‡é¢˜</h1>
            <p class="content">è¿™æ˜¯ç¬¬ä¸€æ®µå†…å®¹</p>
            <p class="content special">è¿™æ˜¯ç‰¹æ®Šçš„ç¬¬äºŒæ®µå†…å®¹</p>
            <ul id="list">
                <li class="item">é¡¹ç›®1</li>
                <li class="item active">é¡¹ç›®2</li>
                <li class="item">é¡¹ç›®3</li>
            </ul>
            <a href="https://example.com" class="link">ç¤ºä¾‹é“¾æ¥</a>
        </div>
    </body>
    </html>
    """
    
    # åˆ›å»ºBeautifulSoupå¯¹è±¡
    soup = BeautifulSoup(html_content, 'html.parser')
    
    print("BeautifulSoupè§£ææ¼”ç¤º:")
    
    # 1. è·å–æ ‡ç­¾å†…å®¹
    title = soup.title
    print(f"1. ç½‘é¡µæ ‡é¢˜: {title.string}")  # è¾“å‡º: ç½‘é¡µæ ‡é¢˜: æµ‹è¯•ç½‘é¡µ
    
    # 2. é€šè¿‡æ ‡ç­¾åæŸ¥æ‰¾
    first_p = soup.p  # ç¬¬ä¸€ä¸ªpæ ‡ç­¾
    print(f"2. ç¬¬ä¸€ä¸ªæ®µè½: {first_p.text}")  # è¾“å‡º: ç¬¬ä¸€ä¸ªæ®µè½: è¿™æ˜¯ç¬¬ä¸€æ®µå†…å®¹
    
    # 3. é€šè¿‡classæŸ¥æ‰¾
    content_paragraphs = soup.find_all('p', class_='content')
    print("3. æ‰€æœ‰contentç±»çš„æ®µè½:")
    for i, p in enumerate(content_paragraphs, 1):
        print(f"   æ®µè½{i}: {p.text}")
    # è¾“å‡º:
    # æ®µè½1: è¿™æ˜¯ç¬¬ä¸€æ®µå†…å®¹
    # æ®µè½2: è¿™æ˜¯ç‰¹æ®Šçš„ç¬¬äºŒæ®µå†…å®¹
    
    # 4. é€šè¿‡idæŸ¥æ‰¾
    main_div = soup.find('div', id='main')
    print(f"4. main divçš„å†…å®¹é•¿åº¦: {len(main_div.text)} å­—ç¬¦")
    # è¾“å‡º: main divçš„å†…å®¹é•¿åº¦: 65 å­—ç¬¦
    
    # 5. CSSé€‰æ‹©å™¨
    active_item = soup.select_one('li.item.active')
    print(f"5. æ´»è·ƒé¡¹ç›®: {active_item.text}")  # è¾“å‡º: æ´»è·ƒé¡¹ç›®: é¡¹ç›®2
    
    all_items = soup.select('li.item')
    print("6. æ‰€æœ‰é¡¹ç›®:")
    for item in all_items:
        print(f"   - {item.text}")
    # è¾“å‡º:
    #   - é¡¹ç›®1
    #   - é¡¹ç›®2
    #   - é¡¹ç›®3
    
    # 6. è·å–å±æ€§
    link = soup.find('a')
    print(f"7. é“¾æ¥åœ°å€: {link['href']}")  # è¾“å‡º: é“¾æ¥åœ°å€: https://example.com
    
    # 7. çˆ¶å­èŠ‚ç‚¹å…³ç³»
    ul = soup.find('ul')
    print(f"8. åˆ—è¡¨å­å…ƒç´ æ•°é‡: {len(ul.find_all('li'))}")  # è¾“å‡º: åˆ—è¡¨å­å…ƒç´ æ•°é‡: 3
    
    return soup

# è¿è¡Œè§£ææ¼”ç¤º
soup = beautifulsoup_parsing()
```

### 4.2 lxmlè§£æå’ŒXPath
```python
def lxml_xpath_parsing():
    """lxmlå’ŒXPathè§£ææ¼”ç¤º"""
    
    from lxml import etree
    import requests
    
    # ç¤ºä¾‹HTMLå†…å®¹
    html_content = """
    <html>
    <body>
        <div class="container">
            <h1>äº§å“åˆ—è¡¨</h1>
            <div class="products">
                <div class="product" data-id="1">
                    <h3 class="name">ç¬”è®°æœ¬ç”µè„‘</h3>
                    <span class="price">Â¥5999</span>
                    <div class="tags">
                        <span>ç”µå­</span>
                        <span>ç”µè„‘</span>
                    </div>
                </div>
                <div class="product" data-id="2">
                    <h3 class="name">æ™ºèƒ½æ‰‹æœº</h3>
                    <span class="price">Â¥3999</span>
                    <div class="tags">
                        <span>ç”µå­</span>
                        <span>é€šè®¯</span>
                    </div>
                </div>
            </div>
        </div>
    </body>
    </html>
    """
    
    # åˆ›å»ºlxmlè§£æå™¨
    parser = etree.HTMLParser()
    tree = etree.fromstring(html_content, parser)
    
    print("lxmlå’ŒXPathè§£ææ¼”ç¤º:")
    
    # 1. åŸºæœ¬çš„XPathæŸ¥è¯¢
    title = tree.xpath('//h1/text()')
    print(f"1. é¡µé¢æ ‡é¢˜: {title[0] if title else 'æœªæ‰¾åˆ°'}")
    # è¾“å‡º: é¡µé¢æ ‡é¢˜: äº§å“åˆ—è¡¨
    
    # 2. è·å–æ‰€æœ‰äº§å“åç§°
    product_names = tree.xpath('//div[@class="product"]/h3[@class="name"]/text()')
    print("2. æ‰€æœ‰äº§å“åç§°:")
    for name in product_names:
        print(f"   - {name}")
    # è¾“å‡º:
    #   - ç¬”è®°æœ¬ç”µè„‘
    #   - æ™ºèƒ½æ‰‹æœº
    
    # 3. è·å–äº§å“ä»·æ ¼
    products = tree.xpath('//div[@class="product"]')
    print("3. äº§å“è¯¦ç»†ä¿¡æ¯:")
    for product in products:
        name = product.xpath('.//h3[@class="name"]/text()')[0]
        price = product.xpath('.//span[@class="price"]/text()')[0]
        product_id = product.xpath('./@data-id')[0]
        print(f"   äº§å“ID: {product_id}, åç§°: {name}, ä»·æ ¼: {price}")
    # è¾“å‡º:
    #   äº§å“ID: 1, åç§°: ç¬”è®°æœ¬ç”µè„‘, ä»·æ ¼: Â¥5999
    #   äº§å“ID: 2, åç§°: æ™ºèƒ½æ‰‹æœº, ä»·æ ¼: Â¥3999
    
    # 4. å¤æ‚çš„XPathæŸ¥è¯¢
    electronic_products = tree.xpath('//div[@class="product"][.//span[text()="ç”µå­"]]')
    print(f"4. ç”µå­äº§å“æ•°é‡: {len(electronic_products)}")
    # è¾“å‡º: ç”µå­äº§å“æ•°é‡: 2
    
    # 5. ä½¿ç”¨containså‡½æ•°
    computer_products = tree.xpath('//div[@class="product"][.//span[contains(text(), "ç”µè„‘")]]')
    print(f"5. ç”µè„‘ç›¸å…³äº§å“æ•°é‡: {len(computer_products)}")
    # è¾“å‡º: ç”µè„‘ç›¸å…³äº§å“æ•°é‡: 1
    
    return tree

# è¿è¡Œlxmlè§£ææ¼”ç¤º
lxml_tree = lxml_xpath_parsing()
```

### 4.3 æ­£åˆ™è¡¨è¾¾å¼è§£æ
```python
def regex_parsing():
    """æ­£åˆ™è¡¨è¾¾å¼è§£ææ¼”ç¤º"""
    
    import re
    
    # ç¤ºä¾‹æ–‡æœ¬å†…å®¹
    text_content = """
    è”ç³»äººä¿¡æ¯ï¼š
    å§“åï¼šå¼ ä¸‰ï¼Œç”µè¯ï¼š138-1234-5678ï¼Œé‚®ç®±ï¼šzhangsan@example.com
    å§“åï¼šæå››ï¼Œç”µè¯ï¼š139-8765-4321ï¼Œé‚®ç®±ï¼šlisi@test.com
    å§“åï¼šç‹äº”ï¼Œç”µè¯ï¼š137-5555-6666ï¼Œé‚®ç®±ï¼šwangwu@sample.org
    
    è®¢å•ä¿¡æ¯ï¼š
    è®¢å•å·ï¼šORD20230101001ï¼Œé‡‘é¢ï¼šÂ¥299.00
    è®¢å•å·ï¼šORD20230101002ï¼Œé‡‘é¢ï¼šÂ¥599.00
    è®¢å•å·ï¼šORD20230101003ï¼Œé‡‘é¢ï¼šÂ¥1299.00
    """
    
    print("æ­£åˆ™è¡¨è¾¾å¼è§£ææ¼”ç¤º:")
    
    # 1. æå–æ‰‹æœºå·ç 
    phone_pattern = r'1[3-9]\d-\d{4}-\d{4}'
    phones = re.findall(phone_pattern, text_content)
    print("1. æå–çš„æ‰‹æœºå·ç :")
    for phone in phones:
        print(f"   - {phone}")
    # è¾“å‡º:
    #   - 138-1234-5678
    #   - 139-8765-4321
    #   - 137-5555-6666
    
    # 2. æå–é‚®ç®±åœ°å€
    email_pattern = r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}'
    emails = re.findall(email_pattern, text_content)
    print("2. æå–çš„é‚®ç®±åœ°å€:")
    for email in emails:
        print(f"   - {email}")
    # è¾“å‡º:
    #   - zhangsan@example.com
    #   - lisi@test.com
    #   - wangwu@sample.org
    
    # 3. æå–è®¢å•ä¿¡æ¯
    order_pattern = r'è®¢å•å·ï¼š(\w+)ï¼Œé‡‘é¢ï¼šÂ¥(\d+\.?\d*)'
    orders = re.findall(order_pattern, text_content)
    print("3. æå–çš„è®¢å•ä¿¡æ¯:")
    for order_num, amount in orders:
        print(f"   è®¢å•å·: {order_num}, é‡‘é¢: {amount}")
    # è¾“å‡º:
    #   è®¢å•å·: ORD20230101001, é‡‘é¢: 299.00
    #   è®¢å•å·: ORD20230101002, é‡‘é¢: 599.00
    #   è®¢å•å·: ORD20230101003, é‡‘é¢: 1299.00
    
    # 4. ä½¿ç”¨åˆ†ç»„æå–å§“åå’Œç”µè¯
    contact_pattern = r'å§“åï¼š(\w+)ï¼Œç”µè¯ï¼š(\d+-\d+-\d+)'
    contacts = re.findall(contact_pattern, text_content)
    print("4. è”ç³»äººä¿¡æ¯:")
    for name, phone in contacts:
        print(f"   å§“å: {name}, ç”µè¯: {phone}")
    # è¾“å‡º:
    #   å§“å: å¼ ä¸‰, ç”µè¯: 138-1234-5678
    #   å§“å: æå››, ç”µè¯: 139-8765-4321
    #   å§“å: ç‹äº”, ç”µè¯: 137-5555-6666
    
    # 5. æ›¿æ¢æ“ä½œ
    replaced_text = re.sub(r'\d{3}-\d{4}-\d{4}', '***-****-****', text_content)
    print("5. è„±æ•åçš„æ–‡æœ¬ï¼ˆå‰100å­—ç¬¦ï¼‰:")
    print(f"   {replaced_text[:100]}...")
    # è¾“å‡º: è”ç³»äººä¿¡æ¯ï¼š å§“åï¼šå¼ ä¸‰ï¼Œç”µè¯ï¼š***-****-****ï¼Œé‚®ç®±ï¼šzhangsan@example.com ...
    
    return {
        'phones': phones,
        'emails': emails,
        'orders': orders,
        'contacts': contacts
    }

# è¿è¡Œæ­£åˆ™è¡¨è¾¾å¼è§£ææ¼”ç¤º
regex_results = regex_parsing()
```

## 5. æ•°æ®å­˜å‚¨æ–¹æ³•

### 5.1 æ–‡ä»¶å­˜å‚¨ï¼ˆCSVã€JSONã€TXTï¼‰
```python
def file_storage_demo():
    """æ–‡ä»¶å­˜å‚¨æ¼”ç¤º"""
    
    import csv
    import json
    import os
    
    # ç¤ºä¾‹æ•°æ®
    sample_data = [
        {'name': 'å¼ ä¸‰', 'age': 25, 'city': 'åŒ—äº¬', 'salary': 8000},
        {'name': 'æå››', 'age': 30, 'city': 'ä¸Šæµ·', 'salary': 12000},
        {'name': 'ç‹äº”', 'age': 28, 'city': 'å¹¿å·', 'salary': 10000},
        {'name': 'èµµå…­', 'age': 35, 'city': 'æ·±åœ³', 'salary': 15000}
    ]
    
    print("æ–‡ä»¶å­˜å‚¨æ¼”ç¤º:")
    
    # 1. CSVæ–‡ä»¶å­˜å‚¨
    print("1. CSVæ–‡ä»¶å­˜å‚¨:")
    with open('employees.csv', 'w', newline='', encoding='utf-8') as csvfile:
        # åˆ›å»ºCSVå†™å…¥å™¨
        writer = csv.DictWriter(csvfile, fieldnames=['name', 'age', 'city', 'salary'])
        
        # å†™å…¥è¡¨å¤´
        writer.writeheader()
        
        # å†™å…¥æ•°æ®
        writer.writerows(sample_data)
    
    print("   âœ… CSVæ–‡ä»¶å·²åˆ›å»º: employees.csv")
    
    # éªŒè¯CSVæ–‡ä»¶
    with open('employees.csv', 'r', encoding='utf-8') as csvfile:
        content = csvfile.read()
        print("   CSVæ–‡ä»¶å†…å®¹:")
        print("   " + content.replace('\n', '\n   '))
    # è¾“å‡º:
    #   name,age,city,salary
    #   å¼ ä¸‰,25,åŒ—äº¬,8000
    #   æå››,30,ä¸Šæµ·,12000
    #   ç‹äº”,28,å¹¿å·,10000
    #   èµµå…­,35,æ·±åœ³,15000
    
    # 2. JSONæ–‡ä»¶å­˜å‚¨
    print("\n2. JSONæ–‡ä»¶å­˜å‚¨:")
    with open('employees.json', 'w', encoding='utf-8') as jsonfile:
        json.dump(sample_data, jsonfile, ensure_ascii=False, indent=2)
    
    print("   âœ… JSONæ–‡ä»¶å·²åˆ›å»º: employees.json")
    
    # éªŒè¯JSONæ–‡ä»¶
    with open('employees.json', 'r', encoding='utf-8') as jsonfile:
        content = jsonfile.read()
        print("   JSONæ–‡ä»¶å†…å®¹ï¼ˆå‰100å­—ç¬¦ï¼‰:")
        print(f"   {content[:100]}...")
    # è¾“å‡º: [ { "name": "å¼ ä¸‰", "age": 25, "city": "åŒ—äº¬", "salary": 8000 }, ...
    
    # 3. æ–‡æœ¬æ–‡ä»¶å­˜å‚¨
    print("\n3. æ–‡æœ¬æ–‡ä»¶å­˜å‚¨:")
    with open('employees.txt', 'w', encoding='utf-8') as txtfile:
        for person in sample_data:
            line = f"{person['name']},{person['age']},{person['city']},{person['salary']}\n"
            txtfile.write(line)
    
    print("   âœ… æ–‡æœ¬æ–‡ä»¶å·²åˆ›å»º: employees.txt")
    
    # 4. è¯»å–å­˜å‚¨çš„æ•°æ®
    print("\n4. ä»CSVæ–‡ä»¶è¯»å–æ•°æ®:")
    with open('employees.csv', 'r', encoding='utf-8') as csvfile:
        reader = csv.DictReader(csvfile)
        for i, row in enumerate(reader, 1):
            print(f"   ç¬¬{i}è¡Œ: {row}")
    # è¾“å‡º:
    #   ç¬¬1è¡Œ: {'name': 'å¼ ä¸‰', 'age': '25', 'city': 'åŒ—äº¬', 'salary': '8000'}
    #   ç¬¬2è¡Œ: {'name': 'æå››', 'age': '30', 'city': 'ä¸Šæµ·', 'salary': '12000'}
    #   ...
    
    # æ¸…ç†ç”Ÿæˆçš„æ–‡ä»¶
    for filename in ['employees.csv', 'employees.json', 'employees.txt']:
        if os.path.exists(filename):
            os.remove(filename)
            print(f"   å·²æ¸…ç†: {filename}")

file_storage_demo()
```

### 5.2 æ•°æ®åº“å­˜å‚¨ï¼ˆMySQLã€MongoDBã€Redisï¼‰
```python
def database_storage_demo():
    """æ•°æ®åº“å­˜å‚¨æ¼”ç¤º"""
    
    print("æ•°æ®åº“å­˜å‚¨æ¼”ç¤º:")
    
    # 1. MySQLå­˜å‚¨
    print("1. MySQLæ•°æ®åº“å­˜å‚¨:")
    try:
        import pymysql
        
        # è¿æ¥MySQLæ•°æ®åº“ï¼ˆéœ€è¦å…ˆå®‰è£…MySQLå¹¶åˆ›å»ºæ•°æ®åº“ï¼‰
        connection = pymysql.connect(
            host='localhost',
            user='root',
            password='your_password',  # æ›¿æ¢ä¸ºä½ çš„å¯†ç 
            database='test_db',        # æ›¿æ¢ä¸ºä½ çš„æ•°æ®åº“å
            charset='utf8mb4'
        )
        
        with connection.cursor() as cursor:
            # åˆ›å»ºè¡¨
            create_table_sql = """
            CREATE TABLE IF NOT EXISTS employees (
                id INT AUTO_INCREMENT PRIMARY KEY,
                name VARCHAR(100) NOT NULL,
                age INT NOT NULL,
                city VARCHAR(100),
                salary DECIMAL(10, 2),
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
            """
            cursor.execute(create_table_sql)
            
            # æ’å…¥æ•°æ®
            insert_sql = "INSERT INTO employees (name, age, city, salary) VALUES (%s, %s, %s, %s)"
            employees_data = [
                ('å¼ ä¸‰', 25, 'åŒ—äº¬', 8000.00),
                ('æå››', 30, 'ä¸Šæµ·', 12000.00),
                ('ç‹äº”', 28, 'å¹¿å·', 10000.00)
            ]
            cursor.executemany(insert_sql, employees_data)
            
            # æäº¤äº‹åŠ¡
            connection.commit()
            print("   âœ… MySQLæ•°æ®æ’å…¥æˆåŠŸ")
            
            # æŸ¥è¯¢æ•°æ®
            cursor.execute("SELECT * FROM employees")
            results = cursor.fetchall()
            print("   æŸ¥è¯¢ç»“æœ:")
            for row in results:
                print(f"     ID: {row[0]}, å§“å: {row[1]}, å¹´é¾„: {row[2]}, åŸå¸‚: {row[3]}, è–ªèµ„: {row[4]}")
        
    except ImportError:
        print("   âŒ è¯·å…ˆå®‰è£…pymysql: pip install pymysql")
    except Exception as e:
        print(f"   âŒ MySQLæ“ä½œå¤±è´¥: {e}")
    
    # 2. MongoDBå­˜å‚¨
    print("\n2. MongoDBæ•°æ®åº“å­˜å‚¨:")
    try:
        import pymongo
        
        # è¿æ¥MongoDBï¼ˆéœ€è¦å…ˆå®‰è£…MongoDBï¼‰
        client = pymongo.MongoClient("mongodb://localhost:27017/")
        db = client["test_database"]
        collection = db["employees"]
        
        # æ’å…¥æ•°æ®
        employees_data = [
            {"name": "å¼ ä¸‰", "age": 25, "city": "åŒ—äº¬", "salary": 8000},
            {"name": "æå››", "age": 30, "city": "ä¸Šæµ·", "salary": 12000},
            {"name": "ç‹äº”", "age": 28, "city": "å¹¿å·", "salary": 10000}
        ]
        
        result = collection.insert_many(employees_data)
        print(f"   âœ… MongoDBæ•°æ®æ’å…¥æˆåŠŸï¼Œæ’å…¥ID: {result.inserted_ids}")
        
        # æŸ¥è¯¢æ•°æ®
        documents = collection.find({"age": {"$gt": 25}})
        print("   æŸ¥è¯¢å¹´é¾„å¤§äº25çš„å‘˜å·¥:")
        for doc in documents:
            print(f"     å§“å: {doc['name']}, å¹´é¾„: {doc['age']}, åŸå¸‚: {doc['city']}")
        
    except ImportError:
        print("   âŒ è¯·å…ˆå®‰è£…pymongo: pip install pymongo")
    except Exception as e:
        print(f"   âŒ MongoDBæ“ä½œå¤±è´¥: {e}")
    
    # 3. Rediså­˜å‚¨
    print("\n3. Redisæ•°æ®åº“å­˜å‚¨:")
    try:
        import redis
        
        # è¿æ¥Redisï¼ˆéœ€è¦å…ˆå®‰è£…Redisï¼‰
        r = redis.Redis(host='localhost', port=6379, db=0, decode_responses=True)
        
        # å­˜å‚¨å­—ç¬¦ä¸²
        r.set('website', 'example.com')
        r.set('visitor_count', 1000)
        
        # å­˜å‚¨å“ˆå¸Œ
        r.hset('user:1001', 'name', 'å¼ ä¸‰')
        r.hset('user:1001', 'age', 25)
        r.hset('user:1001', 'city', 'åŒ—äº¬')
        
        # å­˜å‚¨åˆ—è¡¨
        r.lpush('recent_visitors', 'ç”¨æˆ·A', 'ç”¨æˆ·B', 'ç”¨æˆ·C')
        
        # è¯»å–æ•°æ®
        website = r.get('website')
        visitor_count = r.get('visitor_count')
        user_name = r.hget('user:1001', 'name')
        recent_visitors = r.lrange('recent_visitors', 0, -1)
        
        print("   Rediså­˜å‚¨çš„æ•°æ®:")
        print(f"     ç½‘ç«™: {website}")
        print(f"     è®¿é—®é‡: {visitor_count}")
        print(f"     ç”¨æˆ·å§“å: {user_name}")
        print(f"     æœ€è¿‘è®¿å®¢: {recent_visitors}")
        
    except ImportError:
        print("   âŒ è¯·å…ˆå®‰è£…redis: pip install redis")
    except Exception as e:
        print(f"   âŒ Redisæ“ä½œå¤±è´¥: {e}")

# è¿è¡Œæ•°æ®åº“å­˜å‚¨æ¼”ç¤º
database_storage_demo()
```

## 6. é«˜çº§æŠ€å·§

### 6.1 åŠ¨æ€é¡µé¢å¤„ç†ï¼ˆSeleniumï¼‰
```python
def selenium_dynamic_page():
    """Seleniumå¤„ç†åŠ¨æ€é¡µé¢æ¼”ç¤º"""
    
    print("SeleniumåŠ¨æ€é¡µé¢å¤„ç†æ¼”ç¤º:")
    
    try:
        from selenium import webdriver
        from selenium.webdriver.common.by import By
        from selenium.webdriver.support.ui import WebDriverWait
        from selenium.webdriver.support import expected_conditions as EC
        from selenium.webdriver.chrome.options import Options
        import time
        
        # é…ç½®Chromeé€‰é¡¹
        chrome_options = Options()
        chrome_options.add_argument('--headless')  # æ— å¤´æ¨¡å¼ï¼Œä¸æ˜¾ç¤ºæµè§ˆå™¨çª—å£
        chrome_options.add_argument('--no-sandbox')
        chrome_options.add_argument('--disable-dev-shm-usage')
        
        # åˆ›å»ºæµè§ˆå™¨é©±åŠ¨
        driver = webdriver.Chrome(options=chrome_options)
        
        try:
            # è®¿é—®ç½‘é¡µ
            driver.get("https://httpbin.org/html")
            print("   âœ… é¡µé¢åŠ è½½æˆåŠŸ")
            
            # ç­‰å¾…å…ƒç´ åŠ è½½
            wait = WebDriverWait(driver, 10)
            title_element = wait.until(
                EC.presence_of_element_located((By.TAG_NAME, "h1"))
            )
            
            # è·å–é¡µé¢æ ‡é¢˜
            title = title_element.text
            print(f"   é¡µé¢æ ‡é¢˜: {title}")
            # è¾“å‡º: é¡µé¢æ ‡é¢˜: HtmLorem Ipsum
            
            # è·å–æ‰€æœ‰æ®µè½
            paragraphs = driver.find_elements(By.TAG_NAME, "p")
            print(f"   æ‰¾åˆ° {len(paragraphs)} ä¸ªæ®µè½")
            for i, p in enumerate(paragraphs, 1):
                print(f"     æ®µè½{i}: {p.text[:50]}...")
            
            # æ‰§è¡ŒJavaScript
            script_result = driver.execute_script("return document.title;")
            print(f"   JavaScriptæ‰§è¡Œç»“æœ: {script_result}")
            
            # æˆªå›¾ï¼ˆå¯é€‰ï¼‰
            # driver.save_screenshot('page_screenshot.png')
            # print("   æˆªå›¾å·²ä¿å­˜")
            
        finally:
            # å…³é—­æµè§ˆå™¨
            driver.quit()
            print("   âœ… æµè§ˆå™¨å·²å…³é—­")
            
    except ImportError:
        print("   âŒ è¯·å…ˆå®‰è£…selenium: pip install selenium")
    except Exception as e:
        print(f"   âŒ Seleniumæ“ä½œå¤±è´¥: {e}")

# è¿è¡ŒSeleniumæ¼”ç¤º
selenium_dynamic_page()
```

### 6.2 çˆ¬è™«æ¡†æ¶ï¼ˆScrapyåŸºç¡€ï¼‰
```python
def scrapy_framework_intro():
    """Scrapyæ¡†æ¶ä»‹ç»"""
    
    print("Scrapyçˆ¬è™«æ¡†æ¶ä»‹ç»:")
    
    # Scrapyé¡¹ç›®ç»“æ„
    project_structure = """
    myproject/
    â”œâ”€â”€ scrapy.cfg              # é¡¹ç›®é…ç½®æ–‡ä»¶
    â””â”€â”€ myproject/              # é¡¹ç›®Pythonæ¨¡å—
        â”œâ”€â”€ __init__.py
        â”œâ”€â”€ items.py           # å®šä¹‰çˆ¬å–çš„æ•°æ®ç»“æ„
        â”œâ”€â”€ middlewares.py     # ä¸­é—´ä»¶
        â”œâ”€â”€ pipelines.py       # æ•°æ®ç®¡é“ï¼Œå¤„ç†çˆ¬å–çš„æ•°æ®
        â”œâ”€â”€ settings.py        # é¡¹ç›®è®¾ç½®
        â””â”€â”€ spiders/           # çˆ¬è™«ç›®å½•
            â”œâ”€â”€ __init__.py
            â””â”€â”€ example.py     # çˆ¬è™«å®ç°
    """
    
    print("1. Scrapyé¡¹ç›®ç»“æ„:")
    print(project_structure)
    
    # ç¤ºä¾‹Scrapyçˆ¬è™«ä»£ç 
    sample_spider_code = """
    import scrapy
    
    class ExampleSpider(scrapy.Spider):
        name = 'example'  # çˆ¬è™«åç§°
        allowed_domains = ['example.com']  # å…è®¸çš„åŸŸå
        start_urls = ['http://example.com/']  # èµ·å§‹URL
        
        def parse(self, response):
            # è§£æå“åº”å†…å®¹
            title = response.css('title::text').get()
            links = response.css('a::attr(href)').getall()
            
            # è¿”å›æå–çš„æ•°æ®
            yield {
                'title': title,
                'link_count': len(links),
                'url': response.url
            }
            
            # è·Ÿè¿›é“¾æ¥
            for link in links[:5]:  # åªè·Ÿè¿›å‰5ä¸ªé“¾æ¥
                yield response.follow(link, callback=self.parse)
    """
    
    print("2. ç¤ºä¾‹Scrapyçˆ¬è™«:")
    print(sample_spider_code)
    
    # Scrapyå¸¸ç”¨å‘½ä»¤
    scrapy_commands = [
        "scrapy startproject myproject     # åˆ›å»ºæ–°é¡¹ç›®",
        "scrapy genspider example example.com  # åˆ›å»ºçˆ¬è™«",
        "scrapy crawl example              # è¿è¡Œçˆ¬è™«",
        "scrapy shell 'http://example.com' # äº¤äº’å¼shell",
        "scrapy crawl example -o data.json # å¯¼å‡ºæ•°æ®åˆ°JSON"
    ]
    
    print("3. Scrapyå¸¸ç”¨å‘½ä»¤:")
    for cmd in scrapy_commands:
        print(f"   {cmd}")

scrapy_framework_intro()
```

### 6.3 åçˆ¬è™«ç­–ç•¥åº”å¯¹
```python
def anti_crawler_strategies():
    """åçˆ¬è™«ç­–ç•¥åº”å¯¹æ–¹æ³•"""
    
    import requests
    import time
    import random
    
    print("åçˆ¬è™«ç­–ç•¥åº”å¯¹æ–¹æ³•:")
    
    # 1. è®¾ç½®éšæœºUser-Agent
    print("1. éšæœºUser-Agent:")
    user_agents = [
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.1 Safari/605.1.15',
        'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.107 Safari/537.36'
    ]
    
    headers = {
        'User-Agent': random.choice(user_agents),
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'zh-CN,zh;q=0.8,en-US;q=0.5,en;q=0.3',
        'Accept-Encoding': 'gzip, deflate',
        'Connection': 'keep-alive',
        'Upgrade-Insecure-Requests': '1',
    }
    
    print(f"   ä½¿ç”¨çš„User-Agent: {headers['User-Agent'][:50]}...")
    
    # 2. è®¾ç½®è¯·æ±‚é—´éš”
    print("\n2. è¯·æ±‚é—´éš”æ§åˆ¶:")
    def random_delay(min_delay=1, max_delay=3):
        """éšæœºå»¶æ—¶å‡½æ•°"""
        delay = random.uniform(min_delay, max_delay)
        print(f"   ç­‰å¾… {delay:.2f} ç§’...")
        time.sleep(delay)
    
    # æ¨¡æ‹Ÿå¤šä¸ªè¯·æ±‚
    for i in range(3):
        print(f"   è¯·æ±‚ {i+1}")
        random_delay(1, 2)
    
    # 3. ä½¿ç”¨ä¼šè¯å’Œcookies
    print("\n3. ä½¿ç”¨ä¼šè¯ä¿æŒ:")
    session = requests.Session()
    session.headers.update(headers)
    
    # 4. å¤„ç†éªŒè¯ç ï¼ˆåŸºæœ¬æ€è·¯ï¼‰
    print("\n4. éªŒè¯ç å¤„ç†ç­–ç•¥:")
    captcha_strategies = [
        "ä½¿ç”¨OCRåº“è¯†åˆ«ç®€å•éªŒè¯ç ",
        "ä½¿ç”¨æ‰“ç å¹³å°APIå¤„ç†å¤æ‚éªŒè¯ç ",
        "äººå·¥å¹²é¢„å¤„ç†ç‰¹åˆ«å¤æ‚çš„éªŒè¯ç ",
        "å°è¯•ç»•è¿‡éªŒè¯ç ï¼ˆå¦‚æœå¯èƒ½ä¸”åˆæ³•ï¼‰"
    ]
    
    for strategy in captcha_strategies:
        print(f"   â€¢ {strategy}")
    
    # 5. IPä»£ç†è½®æ¢
    print("\n5. IPä»£ç†ä½¿ç”¨:")
    proxy_strategies = [
        "å…è´¹ä»£ç†ï¼šå®¹æ˜“å¤±æ•ˆï¼Œé€Ÿåº¦æ…¢",
        "ä»˜è´¹ä»£ç†ï¼šç¨³å®šï¼Œé€Ÿåº¦å¿«",
        "ä»£ç†æ± ï¼šè‡ªåŠ¨ç®¡ç†å¤šä¸ªä»£ç†IP",
        "Torç½‘ç»œï¼šåŒ¿åæ€§å¥½ï¼Œé€Ÿåº¦æ…¢"
    ]
    
    for strategy in proxy_strategies:
        print(f"   â€¢ {strategy}")
    
    # 6. å¤„ç†JavaScriptæ¸²æŸ“
    print("\n6. JavaScriptæ¸²æŸ“å¤„ç†:")
    js_handling_methods = [
        "ä½¿ç”¨Selenium/Playwrightæ¨¡æ‹Ÿæµè§ˆå™¨",
        "åˆ†æAjaxè¯·æ±‚ç›´æ¥è·å–æ•°æ®",
        "ä½¿ç”¨Pyppeteeræ— å¤´æµè§ˆå™¨",
        "å¯»æ‰¾ç§»åŠ¨ç«¯APIï¼ˆé€šå¸¸æ²¡æœ‰JSéªŒè¯ï¼‰"
    ]
    
    for method in js_handling_methods:
        print(f"   â€¢ {method}")
    
    return {
        'headers': headers,
        'strategies': {
            'user_agent': 'éšæœºè½®æ¢User-Agent',
            'delay': 'è®¾ç½®éšæœºè¯·æ±‚é—´éš”',
            'session': 'ä½¿ç”¨ä¼šè¯ä¿æŒçŠ¶æ€',
            'proxy': 'ä½¿ç”¨ä»£ç†IP',
            'captcha': 'éªŒè¯ç è¯†åˆ«å¤„ç†',
            'js': 'JavaScriptæ¸²æŸ“å¤„ç†'
        }
    }

# è¿è¡Œåçˆ¬è™«ç­–ç•¥æ¼”ç¤º
anti_crawler_results = anti_crawler_strategies()
```

## 7. æ˜“é”™ç‚¹å’Œéš¾ç‚¹

### 7.1 å¸¸è§é”™è¯¯å’Œè°ƒè¯•æ–¹æ³•
```python
def common_errors_and_debugging():
    """çˆ¬è™«å¸¸è§é”™è¯¯å’Œè°ƒè¯•æ–¹æ³•"""
    
    import requests
    from bs4 import BeautifulSoup
    
    print("çˆ¬è™«å¸¸è§é”™è¯¯å’Œè°ƒè¯•æ–¹æ³•:")
    
    # 1. ç¼–ç é—®é¢˜
    print("1. ç¼–ç é—®é¢˜:")
    try:
        response = requests.get('https://httpbin.org/encoding/utf8')
        # é”™è¯¯çš„ç¼–ç å¤„ç†
        wrong_text = response.content.decode('gbk')  # é”™è¯¯çš„ç¼–ç 
    except UnicodeDecodeError as e:
        print(f"   âŒ ç¼–ç é”™è¯¯: {e}")
        # æ­£ç¡®çš„ç¼–ç å¤„ç†
        correct_text = response.content.decode('utf-8')
        print("   âœ… ä½¿ç”¨æ­£ç¡®ç¼–ç è§£å†³")
    
    # 2. å…ƒç´ ä¸å­˜åœ¨é”™è¯¯
    print("\n2. å…ƒç´ ä¸å­˜åœ¨é”™è¯¯:")
    html = "<div><p>æµ‹è¯•å†…å®¹</p></div>"
    soup = BeautifulSoup(html, 'html.parser')
    
    # é”™è¯¯çš„åšæ³•
    try:
        missing_element = soup.find('span').text  # spanå…ƒç´ ä¸å­˜åœ¨
    except AttributeError as e:
        print(f"   âŒ å…ƒç´ ä¸å­˜åœ¨é”™è¯¯: {e}")
    
    # æ­£ç¡®çš„åšæ³•
    element = soup.find('span')
    if element:
        text = element.text
        print(f"   æ‰¾åˆ°å…ƒç´ : {text}")
    else:
        print("   âœ… å®‰å…¨åœ°å¤„ç†äº†å…ƒç´ ä¸å­˜åœ¨çš„æƒ…å†µ")
    
    # 3. è¯·æ±‚è¶…æ—¶å’Œé‡è¯•
    print("\n3. è¯·æ±‚è¶…æ—¶å¤„ç†:")
    
    def robust_request(url, retries=3, timeout=5):
        """å¥å£®çš„è¯·æ±‚å‡½æ•°ï¼Œæ”¯æŒé‡è¯•"""
        for attempt in range(retries):
            try:
                response = requests.get(url, timeout=timeout)
                print(f"   âœ… ç¬¬{attempt+1}æ¬¡è¯·æ±‚æˆåŠŸ")
                return response
            except requests.exceptions.Timeout:
                print(f"   âš ï¸ ç¬¬{attempt+1}æ¬¡è¯·æ±‚è¶…æ—¶")
            except requests.exceptions.RequestException as e:
                print(f"   âŒ ç¬¬{attempt+1}æ¬¡è¯·æ±‚å¤±è´¥: {e}")
            
            if attempt < retries - 1:
                wait_time = 2 ** attempt  # æŒ‡æ•°é€€é¿
                print(f"   ç­‰å¾… {wait_time} ç§’åé‡è¯•...")
                time.sleep(wait_time)
        
        print("   âŒ æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥")
        return None
    
    # æµ‹è¯•å¥å£®è¯·æ±‚
    test_response = robust_request('https://httpbin.org/delay/2', timeout=1)
    
    # 4. æ•°æ®è§£æé”™è¯¯
    print("\n4. æ•°æ®è§£æé”™è¯¯å¤„ç†:")
    
    def safe_extract(soup, selector, default=''):
        """å®‰å…¨åœ°æå–å…ƒç´ æ–‡æœ¬"""
        try:
            element = soup.select_one(selector)
            return element.text.strip() if element else default
        except Exception as e:
            print(f"   è§£æé”™è¯¯: {e}")
            return default
    
    # æµ‹è¯•å®‰å…¨æå–
    test_html = "<div><p class='content'>æµ‹è¯•å†…å®¹</p></div>"
    test_soup = BeautifulSoup(test_html, 'html.parser')
    
    content1 = safe_extract(test_soup, '.content', 'é»˜è®¤å†…å®¹')
    content2 = safe_extract(test_soup, '.missing', 'é»˜è®¤å†…å®¹')
    
    print(f"   å­˜åœ¨å…ƒç´ : '{content1}'")
    print(f"   ä¸å­˜åœ¨å…ƒç´ : '{content2}'")
    
    # 5. å†…å­˜æ³„æ¼é—®é¢˜
    print("\n5. å†…å­˜ç®¡ç†:")
    memory_tips = [
        "åŠæ—¶å…³é—­æ–‡ä»¶å¥æŸ„å’Œæ•°æ®åº“è¿æ¥",
        "ä½¿ç”¨ç”Ÿæˆå™¨è€Œä¸æ˜¯åˆ—è¡¨å¤„ç†å¤§é‡æ•°æ®",
        "å®šæœŸæ¸…ç†ä¸éœ€è¦çš„å˜é‡",
        "ä½¿ç”¨åˆ†é¡µæˆ–æµå¼å¤„ç†å¤§æ•°æ®é›†"
    ]
    
    for tip in memory_tips:
        print(f"   â€¢ {tip}")

common_errors_and_debugging()
```

### 7.2 æ€§èƒ½ä¼˜åŒ–éš¾ç‚¹
```python
def performance_optimization():
    """çˆ¬è™«æ€§èƒ½ä¼˜åŒ–éš¾ç‚¹"""
    
    import requests
    import time
    import threading
    from concurrent.futures import ThreadPoolExecutor, as_completed
    
    print("çˆ¬è™«æ€§èƒ½ä¼˜åŒ–éš¾ç‚¹:")
    
    # 1. åŒæ­¥è¯·æ±‚çš„æ€§èƒ½é—®é¢˜
    print("1. åŒæ­¥è¯·æ±‚çš„æ€§èƒ½é—®é¢˜:")
    
    def sync_requests(urls):
        """åŒæ­¥è¯·æ±‚æ¼”ç¤º"""
        start_time = time.time()
        results = []
        
        for url in urls:
            try:
                response = requests.get(url, timeout=5)
                results.append(f"{url}: {response.status_code}")
            except Exception as e:
                results.append(f"{url}: ERROR - {e}")
        
        end_time = time.time()
        print(f"   åŒæ­¥è¯·æ±‚è€—æ—¶: {end_time - start_time:.2f}ç§’")
        return results
    
    # 2. å¤šçº¿ç¨‹å¼‚æ­¥è¯·æ±‚
    print("\n2. å¤šçº¿ç¨‹å¼‚æ­¥è¯·æ±‚ä¼˜åŒ–:")
    
    def fetch_url(url):
        """å•ä¸ªURLè¯·æ±‚å‡½æ•°"""
        try:
            response = requests.get(url, timeout=5)
            return f"{url}: {response.status_code}"
        except Exception as e:
            return f"{url}: ERROR - {e}"
    
    def async_requests(urls, max_workers=5):
        """å¤šçº¿ç¨‹å¼‚æ­¥è¯·æ±‚"""
        start_time = time.time()
        results = []
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            future_to_url = {executor.submit(fetch_url, url): url for url in urls}
            
            # æ”¶é›†ç»“æœ
            for future in as_completed(future_to_url):
                result = future.result()
                results.append(result)
        
        end_time = time.time()
        print(f"   å¼‚æ­¥è¯·æ±‚è€—æ—¶: {end_time - start_time:.2f}ç§’")
        print(f"   ä½¿ç”¨çº¿ç¨‹æ•°: {max_workers}")
        return results
    
    # æµ‹è¯•URLåˆ—è¡¨
    test_urls = ['https://httpbin.org/delay/1'] * 5
    
    print("   æµ‹è¯•åŒæ­¥è¯·æ±‚:")
    sync_results = sync_requests(test_urls)
    
    print("   æµ‹è¯•å¼‚æ­¥è¯·æ±‚:")
    async_results = async_requests(test_urls, max_workers=3)
    
    # 3. è¿æ¥å¤ç”¨
    print("\n3. è¿æ¥å¤ç”¨ä¼˜åŒ–:")
    
    def session_requests(urls):
        """ä½¿ç”¨ä¼šè¯å¤ç”¨è¿æ¥"""
        start_time = time.time()
        results = []
        
        with requests.Session() as session:
            for url in urls:
                try:
                    response = session.get(url, timeout=5)
                    results.append(f"{url}: {response.status_code}")
                except Exception as e:
                    results.append(f"{url}: ERROR - {e}")
        
        end_time = time.time()
        print(f"   ä¼šè¯è¯·æ±‚è€—æ—¶: {end_time - start_time:.2f}ç§’")
        return results
    
    session_results = session_requests(test_urls[:3])
    
    # 4. å†…å­˜ä¼˜åŒ–æŠ€å·§
    print("\n4. å†…å­˜ä¼˜åŒ–æŠ€å·§:")
    memory_optimization_tips = [
        "ä½¿ç”¨ç”Ÿæˆå™¨è¡¨è¾¾å¼ä»£æ›¿åˆ—è¡¨æ¨å¯¼å¼",
        "åŠæ—¶å…³é—­æ•°æ®åº“è¿æ¥å’Œæ–‡ä»¶å¥æŸ„",
        "ä½¿ç”¨æµå¼å¤„ç†å¤§æ–‡ä»¶",
        "å®šæœŸæ¸…ç†ç¼“å­˜å’Œä¸´æ—¶æ•°æ®",
        "ä½¿ç”¨åˆ†å—å¤„ç†å¤§æ•°æ®é›†"
    ]
    
    for tip in memory_optimization_tips:
        print(f"   â€¢ {tip}")
    
    # 5. æ•°æ®å¤„ç†ä¼˜åŒ–
    print("\n5. æ•°æ®å¤„ç†ä¼˜åŒ–:")
    data_processing_tips = [
        "åœ¨è§£ææ—¶ç«‹å³è¿‡æ»¤ä¸éœ€è¦çš„æ•°æ®",
        "ä½¿ç”¨é«˜æ•ˆçš„æ•°æ®ç»“æ„ï¼ˆé›†åˆã€å­—å…¸ï¼‰",
        "æ‰¹é‡å†™å…¥æ•°æ®åº“è€Œä¸æ˜¯é€æ¡æ’å…¥",
        "ä½¿ç”¨å‹ç¼©ç®—æ³•å‡å°‘å­˜å‚¨ç©ºé—´",
        "å»ºç«‹ç´¢å¼•åŠ é€Ÿæ•°æ®æŸ¥è¯¢"
    ]
    
    for tip in data_processing_tips:
        print(f"   â€¢ {tip}")

performance_optimization()
```

## 8. ä½¿ç”¨æŠ€å·§å’Œæœ€ä½³å®è·µ

### 8.1 å®ç”¨æŠ€å·§åˆé›†
```python
def practical_tips_and_tricks():
    """çˆ¬è™«å®ç”¨æŠ€å·§åˆé›†"""
    
    import requests
    from bs4 import BeautifulSoup
    import json
    import re
    import time
    
    print("çˆ¬è™«å®ç”¨æŠ€å·§åˆé›†:")
    
    # 1. æ™ºèƒ½å»¶æ—¶ç­–ç•¥
    print("1. æ™ºèƒ½å»¶æ—¶ç­–ç•¥:")
    
    class SmartDelay:
        def __init__(self, base_delay=1, max_delay=10, backoff_factor=2):
            self.base_delay = base_delay
            self.max_delay = max_delay
            self.backoff_factor = backoff_factor
            self.current_delay = base_delay
        
        def wait(self):
            """æ™ºèƒ½ç­‰å¾…"""
            print(f"   ç­‰å¾… {self.current_delay:.2f} ç§’...")
            time.sleep(self.current_delay)
        
        def increase_delay(self):
            """å¢åŠ ç­‰å¾…æ—¶é—´ï¼ˆé‡åˆ°åçˆ¬è™«æ—¶ï¼‰"""
            self.current_delay = min(
                self.current_delay * self.backoff_factor, 
                self.max_delay
            )
            print(f"   æ£€æµ‹åˆ°åçˆ¬è™«ï¼Œå»¶æ—¶å¢åŠ åˆ° {self.current_delay:.2f} ç§’")
        
        def reset_delay(self):
            """é‡ç½®ç­‰å¾…æ—¶é—´"""
            self.current_delay = self.base_delay
            print("   é‡ç½®å»¶æ—¶æ—¶é—´")
    
    delay_manager = SmartDelay()
    delay_manager.wait()
    delay_manager.increase_delay()
    delay_manager.wait()
    delay_manager.reset_delay()
    
    # 2. æ•°æ®æ¸…æ´—å‡½æ•°
    print("\n2. æ•°æ®æ¸…æ´—å‡½æ•°:")
    
    def clean_text(text):
        """æ¸…æ´—æ–‡æœ¬æ•°æ®"""
        if not text:
            return ""
        
        # ç§»é™¤å¤šä½™ç©ºç™½å­—ç¬¦
        text = re.sub(r'\s+', ' ', text.strip())
        # ç§»é™¤ä¸å¯è§å­—ç¬¦
        text = re.sub(r'[\x00-\x1f\x7f-\x9f]', '', text)
        # æ ‡å‡†åŒ–å¼•å·
        text = text.replace('"', '"').replace(''', "'")
        
        return text
    
    dirty_text = "  è¿™æ˜¯ä¸€æ®µ  æœ‰å¾ˆå¤šç©ºæ ¼çš„\tæ–‡æœ¬\nè¿˜æœ‰æ¢è¡Œç¬¦  "
    clean_result = clean_text(dirty_text)
    print(f"   æ¸…æ´—å‰: '{dirty_text}'")
    print(f"   æ¸…æ´—å: '{clean_result}'")
    
    # 3. URLå¤„ç†å·¥å…·
    print("\n3. URLå¤„ç†å·¥å…·:")
    
    def normalize_url(url, base_domain=None):
        """æ ‡å‡†åŒ–URL"""
        import urllib.parse
        
        # è§£æURL
        parsed = urllib.parse.urlparse(url)
        
        # å¤„ç†ç›¸å¯¹URL
        if not parsed.netloc and base_domain:
            return urllib.parse.urljoin(base_domain, url)
        
        # æ ‡å‡†åŒ–å‚æ•°ï¼ˆæŒ‰å­—æ¯æ’åºï¼‰
        query_params = urllib.parse.parse_qs(parsed.query)
        normalized_query = urllib.parse.urlencode(query_params, doseq=True)
        
        # é‡å»ºURL
        normalized_url = urllib.parse.urlunparse((
            parsed.scheme or 'https',
            parsed.netloc,
            parsed.path,
            parsed.params,
            normalized_query,
            parsed.fragment
        ))
        
        return normalized_url
    
    test_url = "https://example.com/path?b=2&a=1"
    normalized = normalize_url(test_url)
    print(f"   åŸå§‹URL: {test_url}")
    print(f"   æ ‡å‡†åŒ–å: {normalized}")
    
    # 4. æ•°æ®éªŒè¯å‡½æ•°
    print("\n4. æ•°æ®éªŒè¯å‡½æ•°:")
    
    def validate_data(data, rules):
        """éªŒè¯çˆ¬å–çš„æ•°æ®"""
        errors = []
        
        for field, rule in rules.items():
            value = data.get(field)
            
            # æ£€æŸ¥å¿…å¡«å­—æ®µ
            if rule.get('required') and not value:
                errors.append(f"å­—æ®µ '{field}' æ˜¯å¿…å¡«çš„")
                continue
            
            # æ£€æŸ¥æ•°æ®ç±»å‹
            expected_type = rule.get('type')
            if expected_type and value and not isinstance(value, expected_type):
                errors.append(f"å­—æ®µ '{field}' åº”è¯¥æ˜¯ {expected_type.__name__} ç±»å‹")
            
            # æ£€æŸ¥é•¿åº¦é™åˆ¶
            min_len = rule.get('min_length')
            max_len = rule.get('max_length')
            if value and hasattr(value, '__len__'):
                if min_len and len(value) < min_len:
                    errors.append(f"å­—æ®µ '{field}' é•¿åº¦ä¸èƒ½å°äº {min_len}")
                if max_len and len(value) > max_len:
                    errors.append(f"å­—æ®µ '{field}' é•¿åº¦ä¸èƒ½å¤§äº {max_len}")
        
        return len(errors) == 0, errors
    
    # æµ‹è¯•æ•°æ®éªŒè¯
    sample_data = {'name': 'å¼ ä¸‰', 'age': 25, 'email': 'test@example.com'}
    validation_rules = {
        'name': {'required': True, 'type': str, 'min_length': 2},
        'age': {'required': True, 'type': int},
        'email': {'required': False, 'type': str}
    }
    
    is_valid, error_messages = validate_data(sample_data, validation_rules)
    print(f"   æ•°æ®éªŒè¯ç»“æœ: {'é€šè¿‡' if is_valid else 'å¤±è´¥'}")
    if error_messages:
        for error in error_messages:
            print(f"     - {error}")
    
    # 5. é…ç½®ç®¡ç†
    print("\n5. é…ç½®ç®¡ç†:")
    
    class CrawlerConfig:
        """çˆ¬è™«é…ç½®ç®¡ç†ç±»"""
        
        def __init__(self):
            self.settings = {
                'request_timeout': 10,
                'retry_times': 3,
                'delay_range': (1, 3),
                'user_agents': [
                    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
                    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
                ],
                'output_format': 'json',
                'max_pages': 1000
            }
        
        def get_setting(self, key, default=None):
            """è·å–é…ç½®é¡¹"""
            return self.settings.get(key, default)
        
        def update_setting(self, key, value):
            """æ›´æ–°é…ç½®é¡¹"""
            self.settings[key] = value
            print(f"   é…ç½®å·²æ›´æ–°: {key} = {value}")
    
    config = CrawlerConfig()
    print(f"   å½“å‰é…ç½®: è¶…æ—¶={config.get_setting('request_timeout')}ç§’, "
          f"é‡è¯•æ¬¡æ•°={config.get_setting('retry_times')}æ¬¡")

practical_tips_and_tricks()
```

### 8.2 å®Œæ•´çˆ¬è™«é¡¹ç›®ç¤ºä¾‹
```python
def complete_crawler_example():
    """å®Œæ•´çš„çˆ¬è™«é¡¹ç›®ç¤ºä¾‹"""
    
    import requests
    from bs4 import BeautifulSoup
    import csv
    import json
    import time
    import random
    import os
    from urllib.parse import urljoin, urlparse
    
    print("å®Œæ•´çˆ¬è™«é¡¹ç›®ç¤ºä¾‹: å›¾ä¹¦ä¿¡æ¯çˆ¬è™«")
    
    class BookCrawler:
        """å›¾ä¹¦ä¿¡æ¯çˆ¬è™«ç±»"""
        
        def __init__(self, base_url, output_file='books.csv'):
            self.base_url = base_url
            self.output_file = output_file
            self.visited_urls = set()
            self.session = requests.Session()
            
            # è®¾ç½®è¯·æ±‚å¤´
            self.session.headers.update({
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                'Accept-Language': 'zh-CN,zh;q=0.8,en-US;q=0.5,en;q=0.3',
            })
            
            # åˆ›å»ºè¾“å‡ºæ–‡ä»¶
            self.setup_output_file()
        
        def setup_output_file(self):
            """åˆå§‹åŒ–è¾“å‡ºæ–‡ä»¶"""
            with open(self.output_file, 'w', newline='', encoding='utf-8') as f:
                writer = csv.writer(f)
                writer.writerow(['ä¹¦å', 'ä½œè€…', 'ä»·æ ¼', 'è¯„åˆ†', 'æè¿°', 'URL'])
            print(f"âœ… è¾“å‡ºæ–‡ä»¶å·²åˆ›å»º: {self.output_file}")
        
        def random_delay(self):
            """éšæœºå»¶æ—¶"""
            delay = random.uniform(1, 3)
            time.sleep(delay)
        
        def is_valid_url(self, url):
            """æ£€æŸ¥URLæ˜¯å¦æœ‰æ•ˆ"""
            parsed = urlparse(url)
            return bool(parsed.netloc) and bool(parsed.scheme)
        
        def crawl_page(self, url):
            """çˆ¬å–å•ä¸ªé¡µé¢"""
            if url in self.visited_urls:
                return
            
            print(f"ğŸ” çˆ¬å–é¡µé¢: {url}")
            self.visited_urls.add(url)
            
            try:
                response = self.session.get(url, timeout=10)
                response.raise_for_status()
                
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # æå–å›¾ä¹¦ä¿¡æ¯ï¼ˆè¿™é‡Œä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®ï¼Œå®é™…é¡¹ç›®ä¸­éœ€è¦æ ¹æ®å…·ä½“ç½‘ç«™ç»“æ„è°ƒæ•´ï¼‰
                books = self.extract_books(soup, url)
                
                # ä¿å­˜å›¾ä¹¦ä¿¡æ¯
                self.save_books(books)
                
                # å‘ç°æ–°çš„é“¾æ¥
                new_links = self.discover_links(soup, url)
                
                # é€’å½’çˆ¬å–æ–°é“¾æ¥
                for link in new_links[:3]:  # é™åˆ¶çˆ¬å–æ•°é‡ï¼Œé¿å…æ— é™é€’å½’
                    self.random_delay()
                    self.crawl_page(link)
                    
            except Exception as e:
                print(f"âŒ çˆ¬å–å¤±è´¥ {url}: {e}")
        
        def extract_books(self, soup, page_url):
            """æå–å›¾ä¹¦ä¿¡æ¯ï¼ˆæ¨¡æ‹Ÿå‡½æ•°ï¼‰"""
            # åœ¨å®é™…é¡¹ç›®ä¸­ï¼Œè¿™é‡Œéœ€è¦æ ¹æ®å…·ä½“ç½‘ç«™ç»“æ„è°ƒæ•´é€‰æ‹©å™¨
            books = []
            
            # æ¨¡æ‹Ÿæå–ä¸€äº›å›¾ä¹¦ä¿¡æ¯
            # è¿™é‡Œä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®ï¼Œå®é™…é¡¹ç›®ä¸­åº”è¯¥ä»ç½‘é¡µä¸­æå–çœŸå®æ•°æ®
            sample_books = [
                {
                    'title': 'Pythonç¼–ç¨‹ä»å…¥é—¨åˆ°å®è·µ',
                    'author': 'Eric Matthes',
                    'price': 'Â¥89.00',
                    'rating': '4.5',
                    'description': 'ä¸€æœ¬å¾ˆå¥½çš„Pythonå…¥é—¨ä¹¦ç±',
                    'url': page_url
                },
                {
                    'title': 'æµç•…çš„Python',
                    'author': 'Luciano Ramalho', 
                    'price': 'Â¥129.00',
                    'rating': '4.8',
                    'description': 'æ·±å…¥ç†è§£Pythonç‰¹æ€§',
                    'url': page_url
                }
            ]
            
            # åœ¨å®é™…é¡¹ç›®ä¸­ï¼Œåº”è¯¥æ˜¯è¿™æ ·æå–ï¼š
            # book_elements = soup.select('.book-item')
            # for element in book_elements:
            #     title = element.select_one('.title').text.strip()
            #     author = element.select_one('.author').text.strip()
            #     ...
            
            return sample_books
        
        def discover_links(self, soup, base_url):
            """å‘ç°æ–°çš„é“¾æ¥"""
            links = []
            for link in soup.find_all('a', href=True):
                href = link['href']
                full_url = urljoin(base_url, href)
                
                if (self.is_valid_url(full_url) and 
                    full_url not in self.visited_urls and
                    self.base_url in full_url):  # åªçˆ¬å–åŒåŸŸåä¸‹çš„é“¾æ¥
                    links.append(full_url)
            
            return links
        
        def save_books(self, books):
            """ä¿å­˜å›¾ä¹¦ä¿¡æ¯åˆ°CSVæ–‡ä»¶"""
            with open(self.output_file, 'a', newline='', encoding='utf-8') as f:
                writer = csv.writer(f)
                for book in books:
                    writer.writerow([
                        book['title'],
                        book['author'], 
                        book['price'],
                        book['rating'],
                        book['description'],
                        book['url']
                    ])
            
            print(f"âœ… å·²ä¿å­˜ {len(books)} æœ¬å›¾ä¹¦ä¿¡æ¯")
        
        def run(self, start_urls):
            """è¿è¡Œçˆ¬è™«"""
            print("ğŸš€ å¼€å§‹çˆ¬å–å›¾ä¹¦ä¿¡æ¯...")
            start_time = time.time()
            
            for url in start_urls:
                self.crawl_page(url)
                self.random_delay()
            
            end_time = time.time()
            print(f"âœ… çˆ¬å–å®Œæˆ! æ€»è€—æ—¶: {end_time - start_time:.2f}ç§’")
            print(f"ğŸ“Š æ€»å…±çˆ¬å–äº† {len(self.visited_urls)} ä¸ªé¡µé¢")
            
            # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
            self.show_statistics()
        
        def show_statistics(self):
            """æ˜¾ç¤ºçˆ¬å–ç»Ÿè®¡ä¿¡æ¯"""
            if os.path.exists(self.output_file):
                with open(self.output_file, 'r', encoding='utf-8') as f:
                    reader = csv.reader(f)
                    row_count = sum(1 for row in reader) - 1  # å‡å»æ ‡é¢˜è¡Œ
                print(f"ğŸ“š æ€»å…±æ”¶é›†äº† {row_count} æœ¬å›¾ä¹¦ä¿¡æ¯")
            else:
                print("âŒ è¾“å‡ºæ–‡ä»¶ä¸å­˜åœ¨")
    
    # ä½¿ç”¨ç¤ºä¾‹
    print("å¼€å§‹è¿è¡Œå®Œæ•´çˆ¬è™«ç¤ºä¾‹...")
    
    # åˆ›å»ºçˆ¬è™«å®ä¾‹
    crawler = BookCrawler(
        base_url='https://httpbin.org',  # ç¤ºä¾‹åŸŸå
        output_file='crawled_books.csv'
    )
    
    # è¿è¡Œçˆ¬è™«ï¼ˆä½¿ç”¨ç¤ºä¾‹URLï¼‰
    start_urls = ['https://httpbin.org/html']
    crawler.run(start_urls)
    
    # æ¸…ç†ç”Ÿæˆçš„æ–‡ä»¶
    if os.path.exists('crawled_books.csv'):
        os.remove('crawled_books.csv')
        print("ğŸ§¹ å·²æ¸…ç†ä¸´æ—¶æ–‡ä»¶")

# è¿è¡Œå®Œæ•´ç¤ºä¾‹
complete_crawler_example()
```

è¿™ä»½è¶…è¯¦ç»†çš„Pythonçˆ¬è™«ç¬”è®°æ¶µç›–äº†ä»åŸºç¡€åˆ°é«˜çº§çš„æ‰€æœ‰é‡è¦çŸ¥è¯†ç‚¹ï¼ŒåŒ…æ‹¬ï¼š

1. **åŸºç¡€æ¦‚å¿µ** - çˆ¬è™«å®šä¹‰ã€æ³•å¾‹è§„èŒƒã€ç¯å¢ƒå‡†å¤‡
2. **å®Œæ•´æµç¨‹** - å·¥ä½œæµç¨‹ã€ç®€å•ç¤ºä¾‹
3. **è¯·æ±‚åº“** - requestsåŸºç¡€ç”¨æ³•ã€é«˜çº§åŠŸèƒ½ã€ä¼šè¯ç®¡ç†
4. **æ•°æ®è§£æ** - BeautifulSoupã€lxml/XPathã€æ­£åˆ™è¡¨è¾¾å¼
5. **æ•°æ®å­˜å‚¨** - æ–‡ä»¶å­˜å‚¨ï¼ˆCSVã€JSONï¼‰ã€æ•°æ®åº“å­˜å‚¨ï¼ˆMySQLã€MongoDBã€Redisï¼‰
6. **é«˜çº§æŠ€å·§** - åŠ¨æ€é¡µé¢å¤„ç†ã€çˆ¬è™«æ¡†æ¶ã€åçˆ¬è™«ç­–ç•¥
7. **æ˜“é”™éš¾ç‚¹** - å¸¸è§é”™è¯¯ã€è°ƒè¯•æ–¹æ³•ã€æ€§èƒ½ä¼˜åŒ–
8. **å®ç”¨æŠ€å·§** - æ™ºèƒ½å»¶æ—¶ã€æ•°æ®æ¸…æ´—ã€é…ç½®ç®¡ç†ã€å®Œæ•´é¡¹ç›®ç¤ºä¾‹

æ¯ä¸ªéƒ¨åˆ†éƒ½åŒ…å«äº†è¯¦ç»†çš„ä»£ç æ³¨é‡Šã€æ˜“é”™ç‚¹è¯´æ˜å’Œä½¿ç”¨æŠ€å·§ï¼Œç¡®ä¿åˆå­¦è€…èƒ½å¤Ÿå…¨é¢ç†è§£å’ŒæŒæ¡Pythonçˆ¬è™«å¼€å‘ã€‚