# Python进程编程超详细笔记

## 1. 进程基础概念

### 1.1 什么是进程
```python
"""
进程：程序的一次执行过程，是系统进行资源分配和调度的基本单位
每个进程都有自己独立的内存空间和系统资源
"""

import os
import time

# 获取当前进程信息
print(f"当前进程ID: {os.getpid()}")           # 当前进程ID: 1234
print(f"父进程ID: {os.getppid()}")           # 父进程ID: 567
print(f"当前工作目录: {os.getcwd()}")        # 当前工作目录: /home/user

# 演示进程独立性
global_var = "初始值"

def show_process_info():
    print(f"函数内进程ID: {os.getpid()}")
    print(f"函数内全局变量: {global_var}")

show_process_info()
# 函数内进程ID: 1234
# 函数内全局变量: 初始值
```

### 1.2 进程与线程的区别
```python
"""
进程 vs 线程：
- 进程：资源分配的基本单位，有独立内存空间，创建销毁开销大
- 线程：CPU调度的基本单位，共享进程内存，创建销毁开销小
- 多进程更稳定，一个进程崩溃不影响其他进程
- 多线程共享数据方便，但需要处理同步问题
"""

import threading

def thread_worker():
    """线程工作函数"""
    print(f"线程ID: {threading.get_ident()}, 进程ID: {os.getpid()}")

# 在主线程中执行
print(f"主线程ID: {threading.get_ident()}, 进程ID: {os.getpid()}")
# 主线程ID: 140235, 进程ID: 1234

# 创建新线程
thread = threading.Thread(target=thread_worker)
thread.start()
thread.join()
# 线程ID: 140236, 进程ID: 1234 （同一进程，不同线程）
```

## 2. 使用multiprocessing模块创建进程

### 2.1 基本进程创建
```python
import multiprocessing
import time
import os

def worker(name, duration):
    """简单的进程工作函数"""
    print(f"进程 {name} (PID: {os.getpid()}) 开始工作")
    time.sleep(duration)
    print(f"进程 {name} 完成工作")
    return f"{name}_结果"

# 创建进程
if __name__ == "__main__":
    print(f"主进程 PID: {os.getpid()}")
    # 主进程 PID: 1234
    
    # 创建进程对象
    process1 = multiprocessing.Process(target=worker, args=("Worker1", 2))
    process2 = multiprocessing.Process(target=worker, args=("Worker2", 1))
    
    # 启动进程
    process1.start()
    process2.start()
    
    print("主进程继续执行...")
    # 主进程继续执行...
    
    # 等待进程结束
    process1.join()
    process2.join()
    
    print("所有子进程执行完毕")
    # 进程 Worker2 (PID: 1236) 开始工作
    # 进程 Worker1 (PID: 1235) 开始工作
    # 进程 Worker2 完成工作
    # 进程 Worker1 完成工作
    # 所有子进程执行完毕
```

### 2.2 进程间内存隔离
```python
import multiprocessing

# 全局变量
shared_list = []

def add_to_list(process_name):
    """演示进程间内存隔离"""
    shared_list.append(process_name)
    print(f"进程 {process_name}: 列表内容: {shared_list}")

if __name__ == "__main__":
    print("=== 演示进程间内存隔离 ===")
    
    # 创建多个进程
    processes = []
    for i in range(3):
        p = multiprocessing.Process(target=add_to_list, args=(f"Process-{i}",))
        processes.append(p)
        p.start()
    
    for p in processes:
        p.join()
    
    # 主进程中的列表
    print(f"主进程中的列表: {shared_list}")
    
    # 输出结果：
    # 进程 Process-0: 列表内容: ['Process-0']
    # 进程 Process-1: 列表内容: ['Process-1'] 
    # 进程 Process-2: 列表内容: ['Process-2']
    # 主进程中的列表: []  # 注意：子进程修改不会影响主进程
```

## 3. 进程间通信(IPC)

### 3.1 使用Queue进行进程间通信
```python
import multiprocessing
import time
import random

def producer(queue, name):
    """生产者进程：向队列中添加数据"""
    for i in range(5):
        item = f"{name}_产品_{i}"
        queue.put(item)
        print(f"生产者 {name} 生产了: {item}")
        time.sleep(random.uniform(0.1, 0.5))
    
    # 发送结束信号
    queue.put(None)

def consumer(queue, name):
    """消费者进程：从队列中取出数据"""
    while True:
        item = queue.get()
        if item is None:
            # 收到结束信号
            queue.put(None)  # 传递给其他消费者
            print(f"消费者 {name} 结束工作")
            break
        
        print(f"消费者 {name} 消费了: {item}")
        time.sleep(random.uniform(0.1, 0.3))

if __name__ == "__main__":
    # 创建队列
    queue = multiprocessing.Queue(maxsize=10)
    
    # 创建生产者进程
    producers = [
        multiprocessing.Process(target=producer, args=(queue, f"P{i}"))
        for i in range(2)
    ]
    
    # 创建消费者进程
    consumers = [
        multiprocessing.Process(target=consumer, args=(queue, f"C{i}"))
        for i in range(3)
    ]
    
    # 启动所有进程
    for p in producers + consumers:
        p.start()
    
    # 等待生产者完成
    for p in producers:
        p.join()
    
    # 等待消费者完成
    for c in consumers:
        c.join()
    
    print("所有进程执行完毕")
    
    # 输出示例：
    # 生产者 P0 生产了: P0_产品_0
    # 消费者 C0 消费了: P0_产品_0
    # 生产者 P1 生产了: P1_产品_0
    # 消费者 C1 消费了: P1_产品_0
    # ...（后续输出）
    # 消费者 C2 结束工作
    # 所有进程执行完毕
```

### 3.2 使用Pipe进行进程间通信
```python
import multiprocessing
import time

def child_process(conn, process_id):
    """子进程函数，通过管道通信"""
    print(f"子进程 {process_id} 启动")
    
    # 从父进程接收消息
    message = conn.recv()
    print(f"子进程 {process_id} 收到: {message}")
    
    # 向父进程发送响应
    response = f"子进程 {process_id} 的响应"
    conn.send(response)
    
    # 关闭连接
    conn.close()
    print(f"子进程 {process_id} 结束")

if __name__ == "__main__":
    print("=== 使用Pipe进行进程间通信 ===")
    
    # 创建管道，返回两个连接对象
    parent_conn, child_conn = multiprocessing.Pipe()
    
    # 创建子进程
    process = multiprocessing.Process(
        target=child_process, 
        args=(child_conn, "Child-1")
    )
    process.start()
    
    # 父进程向子进程发送消息
    parent_conn.send("Hello from parent!")
    
    # 父进程接收子进程的响应
    response = parent_conn.recv()
    print(f"父进程收到: {response}")
    
    # 等待子进程结束
    process.join()
    
    # 关闭父进程连接
    parent_conn.close()
    
    print("通信完成")
    # 输出：
    # === 使用Pipe进行进程间通信 ===
    # 子进程 Child-1 启动
    # 子进程 Child-1 收到: Hello from parent!
    # 父进程收到: 子进程 Child-1 的响应
    # 子进程 Child-1 结束
    # 通信完成
```

### 3.3 使用共享内存进行进程间通信
```python
import multiprocessing
import time

def worker_with_value(shared_value, process_id):
    """使用Value共享内存的进程"""
    for i in range(5):
        with shared_value.get_lock():  # 获取锁
            shared_value.value += 1
            current_value = shared_value.value
        print(f"进程 {process_id} 增加值到: {current_value}")
        time.sleep(0.1)

def worker_with_array(shared_array, process_id):
    """使用Array共享内存的进程"""
    for i in range(len(shared_array)):
        with shared_array.get_lock():  # 获取锁
            shared_array[i] += process_id
        print(f"进程 {process_id} 修改数组索引 {i}")
        time.sleep(0.1)

if __name__ == "__main__":
    print("=== 使用共享内存进行进程间通信 ===")
    
    # 创建共享值（整数，初始值为0）
    shared_value = multiprocessing.Value('i', 0)
    
    # 创建共享数组（5个整数，初始值为0）
    shared_array = multiprocessing.Array('i', 5)
    
    # 创建进程
    processes = []
    
    # 使用Value的进程
    for i in range(2):
        p = multiprocessing.Process(
            target=worker_with_value, 
            args=(shared_value, i)
        )
        processes.append(p)
    
    # 使用Array的进程
    for i in range(2, 4):
        p = multiprocessing.Process(
            target=worker_with_array, 
            args=(shared_array, i)
        )
        processes.append(p)
    
    # 启动所有进程
    for p in processes:
        p.start()
    
    # 等待所有进程完成
    for p in processes:
        p.join()
    
    # 输出最终结果
    print(f"最终共享值: {shared_value.value}")
    print(f"最终共享数组: {list(shared_array)}")
    
    # 输出示例：
    # === 使用共享内存进行进程间通信 ===
    # 进程 0 增加值到: 1
    # 进程 1 增加值到: 2
    # 进程 2 修改数组索引 0
    # 进程 3 修改数组索引 0
    # ...（后续输出）
    # 最终共享值: 10
    # 最终共享数组: [5, 5, 5, 5, 5]
```

### 3.4 使用Manager进行复杂数据共享
```python
import multiprocessing
import time

def worker_with_manager(shared_dict, shared_list, process_id):
    """使用Manager共享复杂数据的进程"""
    # 修改共享字典
    shared_dict[process_id] = f"数据来自进程{process_id}"
    
    # 修改共享列表
    for i in range(3):
        shared_list.append(f"进程{process_id}_项目{i}")
        time.sleep(0.1)
    
    print(f"进程 {process_id} 完成数据修改")

if __name__ == "__main__":
    print("=== 使用Manager共享复杂数据 ===")
    
    # 创建Manager
    with multiprocessing.Manager() as manager:
        # 创建共享字典和列表
        shared_dict = manager.dict()
        shared_list = manager.list()
        
        # 创建进程
        processes = []
        for i in range(3):
            p = multiprocessing.Process(
                target=worker_with_manager,
                args=(shared_dict, shared_list, i)
            )
            processes.append(p)
            p.start()
        
        # 等待所有进程完成
        for p in processes:
            p.join()
        
        # 输出共享数据
        print("最终共享字典:", dict(shared_dict))
        print("最终共享列表:", list(shared_list))
    
    # 输出示例：
    # === 使用Manager共享复杂数据 ===
    # 进程 0 完成数据修改
    # 进程 1 完成数据修改  
    # 进程 2 完成数据修改
    # 最终共享字典: {0: '数据来自进程0', 1: '数据来自进程1', 2: '数据来自进程2'}
    # 最终共享列表: ['进程0_项目0', '进程1_项目0', '进程2_项目0', '进程0_项目1', ...]
```

## 4. 进程同步

### 4.1 使用Lock进行同步
```python
import multiprocessing
import time
import random

def worker_with_lock(lock, shared_value, process_id):
    """使用锁进行同步的进程"""
    for i in range(3):
        # 模拟一些不需要锁的工作
        time.sleep(random.uniform(0.1, 0.3))
        
        # 获取锁（临界区）
        lock.acquire()
        try:
            print(f"进程 {process_id} 获得锁")
            # 临界区操作
            old_value = shared_value.value
            time.sleep(0.1)  # 模拟耗时操作
            shared_value.value = old_value + 1
            print(f"进程 {process_id} 将值从 {old_value} 增加到 {shared_value.value}")
        finally:
            # 释放锁
            lock.release()
            print(f"进程 {process_id} 释放锁")

if __name__ == "__main__":
    print("=== 使用Lock进行进程同步 ===")
    
    # 创建锁和共享值
    lock = multiprocessing.Lock()
    shared_value = multiprocessing.Value('i', 0)
    
    # 创建进程
    processes = []
    for i in range(3):
        p = multiprocessing.Process(
            target=worker_with_lock,
            args=(lock, shared_value, i)
        )
        processes.append(p)
        p.start()
    
    # 等待所有进程完成
    for p in processes:
        p.join()
    
    print(f"最终值: {shared_value.value}")
    
    # 输出示例：
    # === 使用Lock进行进程同步 ===
    # 进程 0 获得锁
    # 进程 0 将值从 0 增加到 1
    # 进程 0 释放锁
    # 进程 1 获得锁
    # 进程 1 将值从 1 增加到 2
    # 进程 1 释放锁
    # ...（后续输出）
    # 最终值: 9
```

### 4.2 使用Semaphore控制并发数量
```python
import multiprocessing
import time
import random

def worker_with_semaphore(semaphore, process_id):
    """使用信号量控制并发数量的进程"""
    print(f"进程 {process_id} 等待信号量...")
    
    # 获取信号量（如果信号量为0则阻塞）
    semaphore.acquire()
    try:
        print(f"进程 {process_id} 获得信号量，开始工作")
        # 模拟工作
        work_time = random.uniform(1, 3)
        time.sleep(work_time)
        print(f"进程 {process_id} 完成工作，耗时 {work_time:.2f} 秒")
    finally:
        # 释放信号量
        semaphore.release()
        print(f"进程 {process_id} 释放信号量")

if __name__ == "__main__":
    print("=== 使用Semaphore控制并发数量 ===")
    
    # 创建信号量，允许最多2个进程同时运行
    semaphore = multiprocessing.Semaphore(2)
    
    # 创建进程
    processes = []
    for i in range(5):
        p = multiprocessing.Process(
            target=worker_with_semaphore,
            args=(semaphore, i)
        )
        processes.append(p)
        p.start()
        time.sleep(0.1)  # 稍微错开启动时间
    
    # 等待所有进程完成
    for p in processes:
        p.join()
    
    print("所有进程完成")
    
    # 输出示例：
    # === 使用Semaphore控制并发数量 ===
    # 进程 0 等待信号量...
    # 进程 0 获得信号量，开始工作
    # 进程 1 等待信号量...
    # 进程 1 获得信号量，开始工作
    # 进程 2 等待信号量...（阻塞）
    # 进程 0 完成工作，耗时 1.23 秒
    # 进程 0 释放信号量
    # 进程 2 获得信号量，开始工作
    # ...（后续输出）
    # 所有进程完成
```

### 4.3 使用Event进行进程间协调
```python
import multiprocessing
import time

def waiter(event, process_id):
    """等待事件的进程"""
    print(f"等待者 {process_id} 等待事件发生...")
    
    # 等待事件被设置
    event.wait()
    
    print(f"等待者 {process_id} 检测到事件已发生，继续执行")

def setter(event, process_id):
    """设置事件的进程"""
    print(f"设置者 {process_id} 正在工作...")
    time.sleep(2)
    
    # 设置事件
    print(f"设置者 {process_id} 设置事件")
    event.set()
    
    print(f"设置者 {process_id} 完成")

if __name__ == "__main__":
    print("=== 使用Event进行进程间协调 ===")
    
    # 创建事件
    event = multiprocessing.Event()
    
    # 创建等待者进程
    waiters = [
        multiprocessing.Process(target=waiter, args=(event, i))
        for i in range(3)
    ]
    
    # 创建设置者进程
    setter_process = multiprocessing.Process(
        target=setter, args=(event, "Setter-1")
    )
    
    # 启动所有进程
    for w in waiters:
        w.start()
    
    time.sleep(0.5)  # 确保等待者先启动
    setter_process.start()
    
    # 等待所有进程完成
    for w in waiters:
        w.join()
    setter_process.join()
    
    print("所有进程完成")
    
    # 输出示例：
    # === 使用Event进行进程间协调 ===
    # 等待者 0 等待事件发生...
    # 等待者 1 等待事件发生...
    # 等待者 2 等待事件发生...
    # 设置者 Setter-1 正在工作...
    # 设置者 Setter-1 设置事件
    # 设置者 Setter-1 完成
    # 等待者 0 检测到事件已发生，继续执行
    # 等待者 1 检测到事件已发生，继续执行
    # 等待者 2 检测到事件已发生，继续执行
    # 所有进程完成
```

## 5. 进程池

### 5.1 使用进程池执行任务
```python
import multiprocessing
import time
import os

def cpu_intensive_task(n):
    """CPU密集型任务"""
    print(f"进程 {os.getpid()} 开始计算 {n}")
    result = sum(i * i for i in range(n))
    time.sleep(0.5)  # 模拟耗时
    print(f"进程 {os.getpid()} 完成计算 {n}")
    return (n, result)

if __name__ == "__main__":
    print("=== 使用进程池执行任务 ===")
    
    # 创建进程池，最多4个进程
    with multiprocessing.Pool(processes=4) as pool:
        # 提交多个任务
        tasks = [100000, 200000, 300000, 400000, 500000]
        
        # 方法1: apply_async - 异步提交
        print("--- 异步提交任务 ---")
        async_results = [
            pool.apply_async(cpu_intensive_task, (task,))
            for task in tasks
        ]
        
        # 获取异步结果
        for result in async_results:
            n, res = result.get()
            print(f"任务 {n} 的结果: {res}")
        
        print("\n--- 使用map方法 ---")
        # 方法2: map - 同步映射
        map_results = pool.map(cpu_intensive_task, [10000, 20000, 30000])
        for n, res in map_results:
            print(f"map任务结果: {n} -> {res}")
        
        print("\n--- 使用imap_unordered方法 ---")
        # 方法3: imap_unordered - 异步迭代器，按完成顺序返回
        for result in pool.imap_unordered(cpu_intensive_task, [5000, 15000, 25000]):
            n, res = result
            print(f"无序返回: {n} -> {res}")
    
    print("所有任务完成")
    
    # 输出示例：
    # === 使用进程池执行任务 ===
    # --- 异步提交任务 ---
    # 进程 1235 开始计算 100000
    # 进程 1236 开始计算 200000
    # 进程 1237 开始计算 300000
    # 进程 1238 开始计算 400000
    # 进程 1235 完成计算 100000
    # 进程 1235 开始计算 500000
    # ...（后续输出）
    # 所有任务完成
```

### 5.2 进程池中的异常处理
```python
import multiprocessing
import traceback

def task_with_possible_error(n):
    """可能出错的任务"""
    if n % 3 == 0:
        raise ValueError(f"数字 {n} 不能被3整除!")
    return n * 2

def safe_worker(n):
    """安全的worker函数，处理异常"""
    try:
        result = task_with_possible_error(n)
        return {"success": True, "result": result, "input": n}
    except Exception as e:
        return {"success": False, "error": str(e), "input": n}

if __name__ == "__main__":
    print("=== 进程池异常处理 ===")
    
    with multiprocessing.Pool(processes=2) as pool:
        numbers = [1, 2, 3, 4, 5, 6, 7, 8, 9]
        
        # 提交任务
        results = pool.map(safe_worker, numbers)
        
        # 处理结果
        for result in results:
            if result["success"]:
                print(f"成功: {result['input']} -> {result['result']}")
            else:
                print(f"失败: {result['input']}, 错误: {result['error']}")
    
    # 输出：
    # === 进程池异常处理 ===
    # 成功: 1 -> 2
    # 成功: 2 -> 4
    # 失败: 3, 错误: 数字 3 不能被3整除!
    # 成功: 4 -> 8
    # 成功: 5 -> 10
    # 失败: 6, 错误: 数字 6 不能被3整除!
    # 成功: 7 -> 14
    # 成功: 8 -> 16
    # 失败: 9, 错误: 数字 9 不能被3整除!
```

## 6. 进程间数据共享的进阶技巧

### 6.1 使用共享内存和NumPy
```python
import multiprocessing
import numpy as np
import time

def numpy_worker(shared_array, shape, process_id):
    """使用NumPy和共享内存的进程"""
    # 将共享数组转换为NumPy数组
    np_array = np.frombuffer(shared_array.get_obj(), dtype=np.float64)
    np_array = np_array.reshape(shape)
    
    # 每个进程处理不同的部分
    chunk_size = shape[0] // 4
    start = process_id * chunk_size
    end = start + chunk_size if process_id < 3 else shape[0]
    
    print(f"进程 {process_id} 处理行 {start} 到 {end-1}")
    
    # 处理数据
    for i in range(start, end):
        for j in range(shape[1]):
            np_array[i, j] = process_id * 100 + i * 10 + j
        time.sleep(0.01)

if __name__ == "__main__":
    print("=== 使用共享内存和NumPy ===")
    
    # 创建共享数组
    shape = (10, 5)
    size = shape[0] * shape[1]
    shared_array = multiprocessing.Array('d', size)  # 'd' 表示double
    
    # 初始化NumPy数组视图
    np_array = np.frombuffer(shared_array.get_obj(), dtype=np.float64)
    np_array = np_array.reshape(shape)
    np_array[:] = 0  # 初始化为0
    
    print("初始数组:")
    print(np_array)
    
    # 创建进程
    processes = []
    for i in range(4):
        p = multiprocessing.Process(
            target=numpy_worker,
            args=(shared_array, shape, i)
        )
        processes.append(p)
        p.start()
    
    # 等待所有进程完成
    for p in processes:
        p.join()
    
    print("\n最终数组:")
    print(np_array)
    
    # 输出示例：
    # === 使用共享内存和NumPy ===
    # 初始数组:
    # [[0. 0. 0. 0. 0.]
    #  ...]
    # 进程 0 处理行 0 到 1
    # 进程 1 处理行 2 到 3
    # ...（后续输出）
    # 最终数组:
    # [[  0.   1.   2.   3.   4.]
    #  [ 10.  11.  12.  13.  14.]
    #  [100. 101. 102. 103. 104.]
    #  ...]
```

## 7. 进程监控和管理

### 7.1 获取进程状态和信息
```python
import multiprocessing
import time
import psutil  # 需要安装: pip install psutil
import os

def long_running_task(process_id, duration):
    """长时间运行的任务"""
    print(f"进程 {process_id} (PID: {os.getpid()}) 开始运行，持续 {duration} 秒")
    start_time = time.time()
    
    # 模拟工作
    while time.time() - start_time < duration:
        # 一些计算工作
        _ = sum(i * i for i in range(10000))
        time.sleep(0.1)
    
    print(f"进程 {process_id} 完成")

def monitor_processes(processes):
    """监控进程状态"""
    print("\n=== 进程监控 ===")
    
    for i, process in enumerate(processes):
        if process.is_alive():
            try:
                # 使用psutil获取详细进程信息
                pid = process.pid
                proc = psutil.Process(pid)
                
                # 获取进程信息
                cpu_percent = proc.cpu_percent()
                memory_info = proc.memory_info()
                status = proc.status()
                
                print(f"进程 {i} (PID: {pid}): "
                      f"CPU: {cpu_percent:.1f}%, "
                      f"内存: {memory_info.rss / 1024 / 1024:.1f}MB, "
                      f"状态: {status}")
            except (psutil.NoSuchProcess, AttributeError):
                print(f"进程 {i}: 无法获取状态信息")
        else:
            print(f"进程 {i}: 已结束，退出码: {process.exitcode}")

if __name__ == "__main__":
    print("=== 进程监控演示 ===")
    
    # 创建多个进程
    processes = []
    for i in range(3):
        duration = i + 2  # 不同的运行时间
        p = multiprocessing.Process(
            target=long_running_task,
            args=(i, duration)
        )
        processes.append(p)
        p.start()
    
    # 监控进程
    for _ in range(5):
        monitor_processes(processes)
        time.sleep(1)
    
    # 等待所有进程完成
    for p in processes:
        p.join()
    
    print("所有进程完成")
    monitor_processes(processes)
    
    # 输出示例：
    # === 进程监控演示 ===
    # 进程 0 (PID: 1235) 开始运行，持续 2 秒
    # 进程 1 (PID: 1236) 开始运行，持续 3 秒
    # 进程 2 (PID: 1237) 开始运行，持续 4 秒
    # 
    # === 进程监控 ===
    # 进程 0 (PID: 1235): CPU: 15.2%, 内存: 12.3MB, 状态: running
    # 进程 1 (PID: 1236): CPU: 12.8%, 内存: 12.1MB, 状态: running
    # 进程 2 (PID: 1237): CPU: 11.5%, 内存: 12.0MB, 状态: running
    # ...（后续监控输出）
    # 所有进程完成
```

## 8. 高级主题：自定义进程类

### 8.1 继承Process类
```python
import multiprocessing
import time
import queue

class WorkerProcess(multiprocessing.Process):
    """自定义进程类"""
    
    def __init__(self, task_queue, result_queue, worker_id):
        super().__init__()
        self.task_queue = task_queue
        self.result_queue = result_queue
        self.worker_id = worker_id
        self.running = True
    
    def run(self):
        """重写run方法，这是进程的入口点"""
        print(f"工作进程 {self.worker_id} 启动")
        
        while self.running:
            try:
                # 从任务队列获取任务（非阻塞）
                task = self.task_queue.get(timeout=1)
                
                if task is None:  # 停止信号
                    print(f"工作进程 {self.worker_id} 收到停止信号")
                    break
                
                # 处理任务
                result = self.process_task(task)
                
                # 将结果放入结果队列
                self.result_queue.put((self.worker_id, task, result))
                
            except queue.Empty:
                # 队列为空，继续等待
                continue
            except Exception as e:
                # 处理异常
                self.result_queue.put((self.worker_id, task, f"错误: {e}"))
        
        print(f"工作进程 {self.worker_id} 结束")
    
    def process_task(self, task):
        """处理具体的任务"""
        print(f"工作进程 {self.worker_id} 处理任务: {task}")
        time.sleep(0.5)  # 模拟处理时间
        
        if isinstance(task, int):
            return task * task
        elif isinstance(task, str):
            return task.upper()
        else:
            return f"处理了: {task}"
    
    def stop(self):
        """停止进程"""
        self.running = False

if __name__ == "__main__":
    print("=== 自定义进程类演示 ===")
    
    # 创建队列
    task_queue = multiprocessing.Queue()
    result_queue = multiprocessing.Queue()
    
    # 创建自定义进程
    workers = [
        WorkerProcess(task_queue, result_queue, i)
        for i in range(3)
    ]
    
    # 启动所有工作进程
    for worker in workers:
        worker.start()
    
    # 添加任务到队列
    tasks = [1, 2, 3, "hello", "world", 4, 5, "python", 6]
    for task in tasks:
        task_queue.put(task)
    
    # 添加停止信号
    for _ in workers:
        task_queue.put(None)
    
    # 收集结果
    results = []
    for _ in tasks:
        try:
            result = result_queue.get(timeout=5)
            results.append(result)
            print(f"收到结果: {result}")
        except queue.Empty:
            break
    
    # 等待所有工作进程结束
    for worker in workers:
        worker.join()
    
    print("\n所有任务完成!")
    print(f"共收到 {len(results)} 个结果")
    
    # 输出示例：
    # === 自定义进程类演示 ===
    # 工作进程 0 启动
    # 工作进程 1 启动
    # 工作进程 2 启动
    # 工作进程 0 处理任务: 1
    # 工作进程 1 处理任务: 2
    # ...（后续输出）
    # 收到结果: (0, 1, 1)
    # 收到结果: (1, 2, 4)
    # ...（后续结果）
    # 所有任务完成!
    # 共收到 9 个结果
```

## 9. 常见易错点和解决方案

### 9.1 全局变量问题
```python
import multiprocessing

# 全局变量 - 在子进程中不会被共享
global_counter = 0

def problematic_worker():
    """有问题的worker函数 - 使用全局变量"""
    global global_counter
    global_counter += 1
    print(f"子进程中的计数器: {global_counter}")

def correct_worker(shared_counter):
    """正确的worker函数 - 使用共享变量"""
    with shared_counter.get_lock():
        shared_counter.value += 1
    print(f"子进程中的共享计数器: {shared_counter.value}")

if __name__ == "__main__":
    print("=== 全局变量问题演示 ===")
    
    print("\n--- 错误用法 ---")
    # 错误用法：使用全局变量
    processes = []
    for i in range(3):
        p = multiprocessing.Process(target=problematic_worker)
        processes.append(p)
        p.start()
    
    for p in processes:
        p.join()
    
    print(f"主进程中的计数器: {global_counter}")  # 仍然是0！
    
    print("\n--- 正确用法 ---")
    # 正确用法：使用共享变量
    shared_counter = multiprocessing.Value('i', 0)
    
    processes = []
    for i in range(3):
        p = multiprocessing.Process(target=correct_worker, args=(shared_counter,))
        processes.append(p)
        p.start()
    
    for p in processes:
        p.join()
    
    print(f"主进程中的共享计数器: {shared_counter.value}")
    
    # 输出：
    # === 全局变量问题演示 ===
    # 
    # --- 错误用法 ---
    # 子进程中的计数器: 1
    # 子进程中的计数器: 1
    # 子进程中的计数器: 1
    # 主进程中的计数器: 0
    # 
    # --- 正确用法 ---
    # 子进程中的共享计数器: 1
    # 子进程中的共享计数器: 2
    # 子进程中的共享计数器: 3
    # 主进程中的共享计数器: 3
```

### 9.2 死锁和资源竞争
```python
import multiprocessing
import time

def worker_with_deadlock(lock1, lock2, process_id):
    """可能产生死锁的worker函数"""
    print(f"进程 {process_id} 尝试获取锁1")
    lock1.acquire()
    print(f"进程 {process_id} 获得锁1")
    
    time.sleep(0.1)  # 模拟一些工作
    
    print(f"进程 {process_id} 尝试获取锁2")
    lock2.acquire()  # 可能在这里死锁
    print(f"进程 {process_id} 获得锁2")
    
    # 临界区操作
    print(f"进程 {process_id} 在临界区工作")
    time.sleep(0.1)
    
    # 释放锁
    lock2.release()
    lock1.release()
    print(f"进程 {process_id} 完成")

def worker_with_timeout(lock1, lock2, process_id, timeout=2):
    """使用超时避免死锁的worker函数"""
    print(f"进程 {process_id} 尝试获取锁1")
    if lock1.acquire(timeout=timeout):
        try:
            print f"进程 {process_id} 获得锁1")
            
            time.sleep(0.1)
            
            print(f"进程 {process_id} 尝试获取锁2 (超时: {timeout}秒)")
            if lock2.acquire(timeout=timeout):
                try:
                    print(f"进程 {process_id} 获得锁2")
                    
                    # 临界区操作
                    print(f"进程 {process_id} 在临界区工作")
                    time.sleep(0.1)
                    
                finally:
                    lock2.release()
            else:
                print(f"进程 {process_id} 获取锁2超时，放弃并释放锁1")
        finally:
            lock1.release()
    else:
        print(f"进程 {process_id} 获取锁1超时")
    
    print(f"进程 {process_id} 结束")

if __name__ == "__main__":
    print("=== 死锁和解决方案 ===")
    
    # 创建锁
    lock1 = multiprocessing.Lock()
    lock2 = multiprocessing.Lock()
    
    print("\n--- 可能产生死锁的情况 ---")
    try:
        # 创建可能死锁的进程
        p1 = multiprocessing.Process(
            target=worker_with_deadlock, 
            args=(lock1, lock2, "A")
        )
        p2 = multiprocessing.Process(
            target=worker_with_deadlock, 
            args=(lock2, lock1, "B")  # 不同的获取顺序！
        )
        
        p1.start()
        p2.start()
        
        # 等待一段时间后终止（避免永久阻塞）
        p1.join(3)
        p2.join(3)
        
        if p1.is_alive() or p2.is_alive():
            print("检测到可能死锁，终止进程")
            p1.terminate()
            p2.terminate()
            p1.join()
            p2.join()
            
    except Exception as e:
        print(f"错误: {e}")
    
    print("\n--- 使用超时避免死锁 ---")
    # 使用带超时的版本
    p3 = multiprocessing.Process(
        target=worker_with_timeout, 
        args=(lock1, lock2, "C")
    )
    p4 = multiprocessing.Process(
        target=worker_with_timeout, 
        args=(lock2, lock1, "D")
    )
    
    p3.start()
    p4.start()
    
    p3.join()
    p4.join()
    
    print("所有进程完成")
    
    # 输出示例：
    # === 死锁和解决方案 ===
    # 
    # --- 可能产生死锁的情况 ---
    # 进程 A 尝试获取锁1
    # 进程 A 获得锁1
    # 进程 B 尝试获取锁2
    # 进程 B 获得锁2
    # 进程 A 尝试获取锁2
    # 进程 B 尝试获取锁1
    # 检测到可能死锁，终止进程
    # 
    # --- 使用超时避免死锁 ---
    # 进程 C 尝试获取锁1
    # 进程 C 获得锁1
    # 进程 D 尝试获取锁2
    # 进程 D 获得锁2
    # 进程 C 尝试获取锁2 (超时: 2秒)
    # 进程 D 尝试获取锁1 (超时: 2秒)
    # 进程 C 获取锁2超时，放弃并释放锁1
    # 进程 C 结束
    # 进程 D 获得锁1
    # 进程 D 在临界区工作
    # 进程 D 结束
    # 所有进程完成
```

## 10. 性能优化技巧

### 10.1 进程池大小优化
```python
import multiprocessing
import time
import os

def cpu_bound_task(n):
    """CPU密集型任务"""
    return sum(i * i for i in range(n))

def io_bound_task(duration):
    """I/O密集型任务（模拟）"""
    time.sleep(duration)
    return f"休眠了 {duration} 秒"

def benchmark_pool_size(tasks, pool_sizes, task_func, task_name):
    """测试不同进程池大小的性能"""
    print(f"\n=== {task_name}任务性能测试 ===")
    
    for pool_size in pool_sizes:
        print(f"\n测试进程池大小: {pool_size}")
        start_time = time.time()
        
        with multiprocessing.Pool(processes=pool_size) as pool:
            results = pool.map(task_func, tasks)
        
        end_time = time.time()
        duration = end_time - start_time
        print(f"完成时间: {duration:.2f} 秒")
        
        # 显示一些结果（避免输出太多）
        if len(results) > 5:
            print(f"前5个结果: {results[:5]}")
        else:
            print(f"所有结果: {results}")

if __name__ == "__main__":
    print("=== 进程池大小优化 ===")
    
    # 获取CPU核心数
    cpu_count = multiprocessing.cpu_count()
    print(f"系统CPU核心数: {cpu_count}")
    
    # 测试CPU密集型任务
    cpu_tasks = [1000000] * 8  # 8个相同的CPU密集型任务
    pool_sizes = [1, 2, 4, 8, 16]
    benchmark_pool_size(cpu_tasks, pool_sizes, cpu_bound_task, "CPU密集型")
    
    # 测试I/O密集型任务
    io_tasks = [0.1] * 16  # 16个I/O密集型任务
    benchmark_pool_size(io_tasks, pool_sizes, io_bound_task, "I/O密集型")
    
    # 输出示例：
    # === 进程池大小优化 ===
    # 系统CPU核心数: 8
    # 
    # === CPU密集型任务性能测试 ===
    # 
    # 测试进程池大小: 1
    # 完成时间: 8.23 秒
    # 前5个结果: [333333500000, 333333500000, ...]
    # 
    # 测试进程池大小: 2
    # 完成时间: 4.15 秒
    # ...（后续输出）
    # 测试进程池大小: 8
    # 完成时间: 1.12 秒（最优）
    # 
    # === I/O密集型任务性能测试 ===
    # 测试进程池大小: 1
    # 完成时间: 1.60 秒
    # 测试进程池大小: 8  
    # 完成时间: 0.21 秒（最优）
```

这个超详细的Python进程编程笔记涵盖了：

1. 进程基础概念和与线程的区别
2. 使用multiprocessing模块创建进程
3. 进程间通信的各种方式（Queue、Pipe、共享内存、Manager）
4. 进程同步机制（Lock、Semaphore、Event）
5. 进程池的使用和优化
6. 高级数据共享技巧（NumPy集成）
7. 进程监控和管理
8. 自定义进程类
9. 常见易错点和解决方案
10. 性能优化技巧

每个概念都配有详细的代码示例和注释，输出结果以注释形式展示，适合系统学习Python多进程编程。