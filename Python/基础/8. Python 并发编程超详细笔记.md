
## 1. 并发编程基础概念

### 1.1 什么是并发编程

```python
"""
并发编程：同时处理多个任务的能力
想象一下：你一边听音乐（IO操作），一边写文档（CPU操作），一边下载文件（网络IO）
这就是并发 - 看起来多个任务在同时进行

三种主要并发模型：

1. 进程（Process）
   - 操作系统资源分配的基本单位，每个程序运行时都会创建一个或多个进程
   - 每个进程有独立的内存空间，互相隔离，安全性高
   - 进程间通信复杂，创建和销毁开销大
   - 好比：公司里的不同部门，各自有独立的办公室和资源

2. 线程（Thread）
   - CPU调度的基本单位，一个进程可以包含多个线程
   - 同一进程的线程共享内存空间，通信简单
   - 线程间需要处理同步问题（避免数据混乱）
   - 好比：一个部门里的多个员工，共享办公室资源，但需要协调工作

3. 协程（Coroutine）
   - 用户态的轻量级线程，由程序自己控制调度
   - 开销极小，可以创建成千上万个协程
   - 适合IO密集型任务（网络请求、文件读写等）
   - 好比：一个员工同时处理多个任务，在等待时切换做其他事

选择原则：
- CPU密集型（大量计算）：多进程（可以利用多核CPU）
- IO密集型（网络、文件操作）：多线程/协程
- 高并发IO（大量网络请求）：协程（效率最高）
"""
```

### 1.2 全局解释器锁（GIL）

```python
"""
GIL（Global Interpreter Lock）：Python解释器的机制
- 同一时刻只有一个线程可以执行Python字节码
- 这是CPython解释器的特性，不是Python语言的特性

GIL的影响：
- 多线程在CPU密集型任务中无法真正并行，实际上是交替执行
- 多线程在IO密集型任务中仍然有效，因为IO等待时会释放GIL
- 多进程不受GIL影响，因为每个进程有独立的GIL

解决方案：
1. 使用多进程处理CPU密集型任务
2. 使用C扩展（C扩展可以释放GIL）
3. 使用其他解释器（如Jython、IronPython）
4. 使用协程处理IO密集型任务

简单理解：GIL就像只有一个收银台的超市，虽然有很多店员（线程），但同一时间只能有一个店员在收银
"""
```

## 2. 线程（Threading）

### 2.1 线程基础

```python
# 导入必要的模块
import threading  # Python的线程模块
import time       # 时间相关功能
import random     # 生成随机数

# ========== 创建线程的三种方式 ==========

# 方式1：直接创建Thread对象（最常用）
def simple_worker(name):
    """简单的线程任务函数
    name: 线程的名称，用于标识不同的线程
    """
    # 打印线程开始执行的信息
    print(f"线程 {name} 开始执行 - 时间: {time.strftime('%H:%M:%S')}")
    # 模拟工作：让线程休眠2秒钟，模拟执行任务需要时间
    time.sleep(2)
    # 打印线程完成执行的信息
    print(f"线程 {name} 执行完成 - 时间: {time.strftime('%H:%M:%S')}")

# 创建线程对象
# Thread参数说明：
# - target: 线程要执行的函数
# - args: 传给函数的参数（必须是元组）
thread1 = threading.Thread(target=simple_worker, args=("线程1",))
thread2 = threading.Thread(target=simple_worker, args=("线程2",))

print("=== 方式1：直接创建线程 ===")
print("注意观察时间，两个线程几乎是同时开始和结束的")

# 启动线程 - 线程开始执行，但不会阻塞主程序
thread1.start()  # 启动线程1
thread2.start()  # 启动线程2

# 等待线程完成
# join()方法会阻塞主程序，直到线程执行完成
thread1.join()   # 等待线程1完成
thread2.join()   # 等待线程2完成

print("主程序继续执行...\n")

# 方式2：继承Thread类（面向对象的方式）
class MyThread(threading.Thread):
    """自定义线程类
    通过继承Thread类，可以更好地封装线程逻辑
    """
    
    def __init__(self, name, delay):
        """初始化方法
        name: 线程名称
        delay: 模拟工作延迟时间（秒）
        """
        # 调用父类的初始化方法，必须的
        super().__init__()
        # 设置线程名称
        self.name = name
        # 设置延迟时间
        self.delay = delay
    
    def run(self):
        """线程执行的主体方法
        当调用start()方法时，会自动调用此方法
        注意：必须重写这个方法
        """
        print(f"{self.name} 开始执行，延迟 {self.delay:.2f} 秒")
        # 模拟工作：休眠指定时间
        time.sleep(self.delay)
        print(f"{self.name} 执行完成")

print("=== 方式2：继承Thread类 ===")
threads = []  # 用于保存所有线程对象的列表

# 创建3个自定义线程，每个有随机的延迟时间
for i in range(3):
    # 创建线程，延迟时间在1-3秒之间随机
    t = MyThread(f"自定义线程-{i+1}", random.uniform(1, 3))
    threads.append(t)  # 将线程添加到列表
    t.start()         # 启动线程

print("所有线程已启动，等待它们完成...")

# 等待所有线程完成
for t in threads:
    t.join()  # 等待每个线程完成

print("所有自定义线程执行完成\n")

# 方式3：使用线程池（推荐用于大量任务）
from concurrent.futures import ThreadPoolExecutor  # 线程池模块

def pool_worker(task_id):
    """线程池任务函数
    task_id: 任务ID
    """
    # 获取当前线程的名称
    thread_name = threading.current_thread().name
    print(f"任务 {task_id} 在线程 {thread_name} 中执行")
    # 模拟工作：休眠1秒
    time.sleep(1)
    # 返回任务结果
    return f"任务 {task_id} 完成"

print("=== 方式3：使用线程池 ===")
print("线程池会自动管理线程的创建和销毁，更高效")

# 创建线程池，最大工作线程数为3
# with语句确保线程池在使用后正确关闭
with ThreadPoolExecutor(max_workers=3) as executor:
    # 提交5个任务到线程池
    # executor.submit()用于提交单个任务
    futures = []  # 保存Future对象的列表
    
    # 提交5个任务
    for i in range(5):
        # 提交任务，返回Future对象（代表未来的结果）
        future = executor.submit(pool_worker, i)
        futures.append(future)
    
    print("所有任务已提交，正在执行中...")
    
    # 获取所有任务的结果
    for future in futures:
        # future.result()会阻塞，直到任务完成并返回结果
        result = future.result()
        print(f"收到结果: {result}")

print("线程池任务全部完成\n")
```

### 2.2 线程同步

```python
import threading
import time
import random

# ========== 锁（Lock） ==========
print("=== 锁机制：解决多线程数据竞争问题 ===")

class BankAccount:
    """银行账户类 - 演示线程安全问题
    如果没有锁，多个线程同时修改余额会导致数据错误
    """
    
    def __init__(self, initial_balance=1000):
        """初始化账户
        initial_balance: 初始余额
        """
        self.balance = initial_balance
        # 创建锁对象，用于保护余额数据的修改
        self.lock = threading.Lock()
        print(f"创建银行账户，初始余额: {self.balance}")
    
    def withdraw(self, amount, user):
        """取款操作 - 线程不安全版本（演示问题）
        amount: 取款金额
        user: 用户名
        """
        print(f"{user} 尝试取款 {amount}")
        
        # 检查余额是否足够（这里没有加锁，可能产生竞态条件）
        if self.balance >= amount:
            # 模拟处理时间，增加出现问题的概率
            time.sleep(0.1)
            # 修改余额
            self.balance -= amount
            print(f"{user} 取款 {amount} 成功，余额: {self.balance}")
            return True
        else:
            print(f"{user} 取款 {amount} 失败，余额不足")
            return False
    
    def safe_withdraw(self, amount, user):
        """安全的取款操作 - 使用锁保护
        amount: 取款金额
        user: 用户名
        """
        print(f"{user} 尝试安全取款 {amount}")
        
        # 方法1：手动获取和释放锁（不推荐，容易忘记释放）
        # self.lock.acquire()  # 获取锁
        # try:
        #     # 在锁保护下的代码
        #     if self.balance >= amount:
        #         time.sleep(0.1)
        #         self.balance -= amount
        #         print(f"{user} 取款 {amount} 成功，余额: {self.balance}")
        #         return True
        #     else:
        #         print(f"{user} 取款 {amount} 失败，余额不足")
        #         return False
        # finally:
        #     self.lock.release()  # 确保锁被释放
        
        # 方法2：使用with语句（推荐，自动管理锁的获取和释放）
        with self.lock:  # 进入with块时自动获取锁，退出时自动释放
            # 在锁的保护下检查余额和修改余额
            if self.balance >= amount:
                # 模拟处理时间
                time.sleep(0.1)
                # 修改余额
                self.balance -= amount
                print(f"{user} 安全取款 {amount} 成功，余额: {self.balance}")
                return True
            else:
                print(f"{user} 安全取款 {amount} 失败，余额不足")
                return False

# 测试线程安全的银行账户
print("创建银行账户，余额1000")
account = BankAccount(1000)

def user_operation(user_name):
    """用户操作函数
    user_name: 用户名
    """
    # 每个用户尝试取款3次
    for i in range(3):
        # 随机取款金额在100-300之间
        amount = random.randint(100, 300)
        # 调用安全的取款方法
        account.safe_withdraw(amount, user_name)
        # 短暂休眠，让其他线程有机会执行
        time.sleep(0.05)

# 创建多个用户线程
users = ["张三", "李四", "王五"]
threads = []  # 保存线程对象的列表

print(f"启动{len(users)}个用户线程同时取款...")
for user in users:
    # 为每个用户创建线程
    t = threading.Thread(target=user_operation, args=(user,))
    threads.append(t)  # 添加到列表
    t.start()         # 启动线程

# 等待所有线程完成
print("等待所有用户完成取款操作...")
for t in threads:
    t.join()

print(f"最终余额: {account.balance}")
print("注意：如果不用锁，余额可能会出现负数或者计算错误\n")

# ========== 可重入锁（RLock） ==========
print("=== 可重入锁：同一个线程可以多次获取同一个锁 ===")

class Calculator:
    """计算器类 - 演示可重入锁的使用场景
    在递归调用或复杂调用链中，普通锁会导致死锁
    """
    
    def __init__(self):
        # 创建可重入锁（Reentrant Lock）
        # 同一个线程可以多次获取这个锁，不会阻塞自己
        self.rlock = threading.RLock()
        self.value = 0
        print("创建计算器对象")
    
    def calculate(self, x, y):
        """计算操作 - 可能调用其他需要锁的方法
        x, y: 要计算的数值
        """
        # 获取可重入锁
        with self.rlock:
            print(f"开始计算: {x} + {y}")
            # 调用内部方法，内部方法也需要获取同一个锁
            # 如果是普通Lock，这里会导致死锁（线程会等待自己释放锁）
            self.value = self._add(x, y)
            print(f"计算完成，结果: {self.value}")
            return self.value
    
    def _add(self, a, b):
        """内部加法方法
        a, b: 要相加的数值
        """
        # 再次获取同一个锁（可重入锁允许这样做）
        with self.rlock:
            result = a + b
            print(f"执行加法: {a} + {b} = {result}")
            return result

# 测试可重入锁
calc = Calculator()
# 创建线程执行计算任务
thread = threading.Thread(target=calc.calculate, args=(5, 3))
thread.start()
thread.join()

print("可重入锁允许同一个线程多次获取锁，避免了死锁\n")

# ========== 条件变量（Condition） ==========
print("=== 条件变量：线程间的等待和通知机制 ===")

class MessageQueue:
    """消息队列 - 演示条件变量的使用
    生产者线程生产消息，消费者线程消费消息
    当队列满时生产者等待，当队列空时消费者等待
    """
    
    def __init__(self, max_size=5):
        """初始化消息队列
        max_size: 队列最大容量
        """
        self.queue = []           # 消息队列（列表）
        self.max_size = max_size  # 最大容量
        # 创建条件变量，内部包含一个锁
        self.condition = threading.Condition()
        print(f"创建消息队列，最大容量: {max_size}")
    
    def produce(self, message, producer_name):
        """生产消息
        message: 消息内容
        producer_name: 生产者名称
        """
        # 获取条件变量的锁
        with self.condition:
            # 检查队列是否已满，如果满就等待
            while len(self.queue) >= self.max_size:
                print(f"{producer_name} 等待，队列已满（当前大小: {len(self.queue)}）")
                # 等待，会释放锁，让其他线程运行
                # 当被notify唤醒时，会重新获取锁
                self.condition.wait()
            
            # 生产消息（队列不满时执行）
            self.queue.append(message)
            print(f"{producer_name} 生产: {message}，队列大小: {len(self.queue)}")
            
            # 通知所有等待的消费者线程
            self.condition.notify_all()
    
    def consume(self, consumer_name):
        """消费消息
        consumer_name: 消费者名称
        返回: 消费的消息
        """
        # 获取条件变量的锁
        with self.condition:
            # 检查队列是否为空，如果空就等待
            while len(self.queue) == 0:
                print(f"{consumer_name} 等待，队列为空")
                # 等待生产者生产消息
                self.condition.wait()
            
            # 消费消息（队列不空时执行）
            message = self.queue.pop(0)  # 从队列开头取出消息
            print(f"{consumer_name} 消费: {message}，队列大小: {len(self.queue)}")
            
            # 通知所有等待的生产者线程
            self.condition.notify_all()
            
            return message

# 测试生产者消费者模式
print("创建容量为3的消息队列")
mq = MessageQueue(3)

def producer_task(producer_name, message_count):
    """生产者任务函数
    producer_name: 生产者名称
    message_count: 要生产的消息数量
    """
    for i in range(message_count):
        # 生成消息
        message = f"消息-{producer_name}-{i+1}"
        # 生产消息
        mq.produce(message, producer_name)
        # 随机休眠，模拟生产时间
        time.sleep(random.uniform(0.1, 0.3))

def consumer_task(consumer_name, message_count):
    """消费者任务函数
    consumer_name: 消费者名称
    message_count: 要消费的消息数量
    """
    for i in range(message_count):
        # 消费消息
        message = mq.consume(consumer_name)
        # 随机休眠，模拟消费时间
        time.sleep(random.uniform(0.2, 0.4))

# 创建生产者和消费者线程
producers = [
    threading.Thread(target=producer_task, args=("生产者A", 3)),
    threading.Thread(target=producer_task, args=("生产者B", 3))
]

consumers = [
    threading.Thread(target=consumer_task, args=("消费者1", 3)),
    threading.Thread(target=consumer_task, args=("消费者2", 3))
]

print("启动2个生产者和2个消费者...")

# 启动所有线程
for p in producers:
    p.start()

for c in consumers:
    c.start()

# 等待生产者完成
print("等待生产者完成...")
for p in producers:
    p.join()

# 等待消费者完成
print("等待消费者完成...")
for c in consumers:
    c.join()

print("生产者消费者模式演示完成\n")
```

### 2.3 线程通信

```python
import threading
import queue  # 线程安全的队列模块
import time
import random

# ========== 队列（Queue） ==========
print("=== 线程安全队列：线程间通信的最佳实践 ===")

def producer_worker(q, producer_name, item_count):
    """生产者工作函数
    q: 队列对象
    producer_name: 生产者名称
    item_count: 要生产的项目数量
    """
    for i in range(item_count):
        # 生成项目
        item = f"{producer_name}-产品-{i+1}"
        # 将项目放入队列（如果队列满，会阻塞直到有空位）
        q.put(item)
        print(f"{producer_name} 生产: {item} (队列大小: {q.qsize()})")
        # 随机休眠，模拟生产时间
        time.sleep(random.uniform(0.1, 0.3))
    
    # 生产完成后，发送结束信号（None）
    print(f"{producer_name} 完成生产，发送结束信号")
    q.put(None)

def consumer_worker(q, consumer_name):
    """消费者工作函数
    q: 队列对象
    consumer_name: 消费者名称
    """
    item_count = 0  # 消费计数器
    while True:
        # 从队列获取项目（如果队列空，会阻塞直到有项目）
        item = q.get()
        
        # 检查是否是结束信号
        if item is None:
            # 将结束信号放回队列，让其他消费者也能结束
            q.put(None)
            print(f"{consumer_name} 收到结束信号，停止工作。共消费 {item_count} 个项目")
            break
        
        # 正常消费项目
        print(f"{consumer_name} 消费: {item} (队列大小: {q.qsize()})")
        # 随机休眠，模拟消费时间
        time.sleep(random.uniform(0.2, 0.4))
        item_count += 1
        
        # 标记任务完成（用于q.join()）
        q.task_done()

# 创建线程安全队列，最大容量为5
# Queue是线程安全的，多个线程可以同时操作而不会出问题
q = queue.Queue(5)
print(f"创建线程安全队列，最大容量: 5")

# 创建生产者和消费者线程
producers = [
    threading.Thread(target=producer_worker, args=(q, "工厂A", 3)),
    threading.Thread(target=producer_worker, args=(q, "工厂B", 3))
]

consumers = [
    threading.Thread(target=consumer_worker, args=(q, "消费者1")),
    threading.Thread(target=consumer_worker, args=(q, "消费者2"))
]

print("启动2个生产者和2个消费者...")

# 启动线程（先启动消费者，这样生产者一生产就能被消费）
for c in consumers:
    c.start()

for p in producers:
    p.start()

# 等待生产者完成
print("等待生产者完成生产...")
for p in producers:
    p.join()

print("所有生产者已完成，等待队列中的任务被消费完...")

# 等待队列中的所有任务被处理完成
# q.join()会阻塞，直到所有通过q.task_done()标记的任务完成
q.join()

print("所有任务已完成，通知消费者结束...")

# 放入结束信号，通知消费者结束
q.put(None)

# 等待消费者结束
for c in consumers:
    c.join()

print("队列通信演示完成\n")

# ========== 事件（Event） ==========
print("=== 事件机制：线程间的简单信号通信 ===")

class WorkerTeam:
    """工作组 - 演示事件机制
    使用事件来控制线程的执行流程
    """
    
    def __init__(self):
        # 开始事件：控制线程何时开始工作
        self.start_event = threading.Event()
        # 暂停事件：控制线程何时暂停工作
        self.pause_event = threading.Event()
        # 停止事件：控制线程何时停止工作
        self.stop_event = threading.Event()
        print("创建工作组，使用事件控制线程")
    
    def worker(self, worker_name):
        """工作者函数
        worker_name: 工作者名称
        """
        print(f"{worker_name} 等待开始信号...")
        
        # 等待开始事件（阻塞，直到start_event被设置）
        self.start_event.wait()
        print(f"{worker_name} 收到开始信号，开始工作")
        
        work_count = 0  # 工作计数器
        while not self.stop_event.is_set():  # 检查停止事件
            if self.pause_event.is_set():
                # 如果暂停事件被设置，就等待
                time.sleep(0.1)  # 短暂休眠，避免忙等待
                continue
            
            # 正常工作
            work_count += 1
            print(f"{worker_name} 正在工作，完成第 {work_count} 项任务")
            time.sleep(0.5)  # 模拟工作时间
            
            # 完成10项任务后自动停止（演示用）
            if work_count >= 10:
                print(f"{worker_name} 已完成10项任务，自动停止")
                break
        
        print(f"{worker_name} 工作结束")

# 测试事件机制
print("创建工作组")
team = WorkerTeam()

# 创建工作线程
workers = []
worker_names = ["工人-甲", "工人-乙", "工人-丙"]

for name in worker_names:
    t = threading.Thread(target=team.worker, args=(name,))
    workers.append(t)
    t.start()

print("3秒后发送开始信号...")
time.sleep(3)

# 设置开始事件，所有等待的worker线程开始工作
print("发送开始信号!")
team.start_event.set()

# 让工人工作2秒
time.sleep(2)

# 设置暂停事件，暂停工作
print("发送暂停信号!")
team.pause_event.set()
time.sleep(2)

# 清除暂停事件，恢复工作
print("发送恢复信号!")
team.pause_event.clear()
time.sleep(2)

# 设置停止事件，停止所有工作
print("发送停止信号!")
team.stop_event.set()

# 等待所有工作线程结束
print("等待所有工人结束工作...")
for w in workers:
    w.join()

print("事件机制演示完成\n")

# ========== 信号量（Semaphore） ==========
print("=== 信号量：控制同时访问资源的线程数量 ===")

class ResourcePool:
    """资源池 - 演示信号量
    限制同时使用资源的线程数量
    """
    
    def __init__(self, total_resources=3):
        """初始化资源池
        total_resources: 资源总数，也是信号量的初始值
        """
        # 创建信号量，控制最多同时有3个线程访问资源
        self.semaphore = threading.Semaphore(total_resources)
        # 可用资源列表
        self.resources = list(range(total_resources))
        # 锁，用于保护资源列表的修改
        self.lock = threading.Lock()
        print(f"创建资源池，共有 {total_resources} 个资源: {self.resources}")
    
    def use_resource(self, user_name):
        """使用资源
        user_name: 使用者名称
        """
        print(f"{user_name} 等待可用资源...")
        
        # 获取信号量（如果没有可用资源，会阻塞等待）
        # 信号量计数器减1
        self.semaphore.acquire()
        
        try:
            # 获取资源（需要锁保护，因为多个线程可能同时修改资源列表）
            with self.lock:
                resource_id = self.resources.pop()  # 取出一个资源
                print(f"{user_name} 获取资源 {resource_id} (剩余资源: {self.resources})")
            
            # 模拟使用资源（这里不需要锁，因为每个线程有自己的资源）
            print(f"{user_name} 正在使用资源 {resource_id}...")
            time.sleep(random.uniform(1, 2))  # 使用资源1-2秒
            print(f"{user_name} 使用资源 {resource_id} 完成")
            
        finally:
            # 无论是否发生异常，都要释放资源
            with self.lock:
                # 将资源放回池中
                self.resources.append(resource_id)
                print(f"{user_name} 释放资源 {resource_id} (可用资源: {self.resources})")
            
            # 释放信号量（信号量计数器加1）
            self.semaphore.release()

def user_task(user_id):
    """用户任务函数
    user_id: 用户ID
    """
    user_name = f"用户-{user_id}"
    # 使用资源
    pool.use_resource(user_name)

# 测试资源池
print("创建有3个资源的资源池")
pool = ResourcePool(3)

# 创建8个用户线程（超过资源数量，有些需要等待）
user_threads = []
print("创建8个用户线程（超过3个资源），有些需要等待...")

for i in range(8):
    t = threading.Thread(target=user_task, args=(i+1,))
    user_threads.append(t)
    t.start()
    # 稍微错开启动时间
    time.sleep(0.1)

# 等待所有用户线程完成
print("等待所有用户完成资源使用...")
for t in user_threads:
    t.join()

print("信号量演示完成")
print("注意：虽然创建了8个线程，但同一时间只有3个能使用资源\n")
```

## 3. 进程（Multiprocessing）

### 3.1 进程基础

```python
# 导入多进程模块
import multiprocessing  # Python的多进程模块
import time
import os  # 操作系统接口模块
import random

# ========== 创建进程 ==========
print("=== 多进程基础：理解进程的创建和管理 ===")

def process_worker(process_name, duration):
    """进程工作函数
    process_name: 进程名称
    duration: 工作持续时间（秒）
    """
    # 获取当前进程的ID和父进程ID
    current_pid = os.getpid()      # 当前进程ID
    parent_pid = os.getppid()      # 父进程ID
    print(f"进程 {process_name} (PID: {current_pid}) 开始执行，父进程PID: {parent_pid}")
    
    # 模拟工作：休眠指定时间
    time.sleep(duration)
    
    print(f"进程 {process_name} (PID: {current_pid}) 执行完成")
    return f"{process_name}-结果"

# 重要：在Windows系统中，multiprocessing必须放在if __name__ == '__main__'中
# 这是因为Windows没有fork，需要重新导入模块
if __name__ == "__main__":
    print("主进程开始 (PID: {})".format(os.getpid()))
    
    # 方式1：直接创建Process对象（最常用）
    # 创建两个进程对象
    p1 = multiprocessing.Process(
        target=process_worker,  # 进程要执行的函数
        args=("进程1", 2)       # 传给函数的参数
    )
    p2 = multiprocessing.Process(
        target=process_worker,
        args=("进程2", 3)
    )

    print("启动进程1和进程2...")
    # 启动进程（开始执行）
    p1.start()  # 启动进程1
    p2.start()  # 启动进程2

    print("主进程等待子进程完成...")
    # 等待进程完成（阻塞主进程）
    p1.join()   # 等待进程1完成
    p2.join()   # 等待进程2完成
    
    print("所有子进程完成，主进程继续执行\n")

# 方式2：继承Process类（面向对象的方式）
class MyProcess(multiprocessing.Process):
    """自定义进程类
    通过继承Process类，可以更好地封装进程逻辑
    """
    
    def __init__(self, name, duration):
        """初始化方法
        name: 进程名称
        duration: 工作持续时间
        """
        # 必须调用父类的初始化方法
        super().__init__()
        self.name = name
        self.duration = duration
        print(f"创建自定义进程: {name}")
    
    def run(self):
        """进程执行的主体方法
        当调用start()方法时，会自动调用此方法
        注意：必须重写这个方法
        """
        print(f"自定义进程 {self.name} 开始执行 (PID: {os.getpid()})")
        # 模拟工作
        time.sleep(self.duration)
        print(f"自定义进程 {self.name} 执行完成 (PID: {os.getpid()})")

if __name__ == "__main__":
    print("=== 方式2：继承Process类 ===")
    processes = []  # 保存进程对象的列表
    
    # 创建3个自定义进程
    for i in range(3):
        # 创建进程，随机持续时间1-2秒
        p = MyProcess(f"自定义进程-{i+1}", random.uniform(1, 2))
        processes.append(p)  # 添加到列表
        p.start()           # 启动进程
    
    print("所有自定义进程已启动，等待它们完成...")
    
    # 等待所有进程完成
    for p in processes:
        p.join()  # 等待每个进程完成
    
    print("所有自定义进程执行完成\n")

# 方式3：使用进程池（推荐用于大量任务）
def pool_worker(task_id):
    """进程池工作函数
    task_id: 任务ID
    """
    # 获取当前进程ID
    current_pid = os.getpid()
    print(f"任务 {task_id} 在进程 {current_pid} 中执行")
    # 模拟工作
    time.sleep(1)
    # 返回任务结果
    return f"任务 {task_id} 结果 (来自进程 {current_pid})"

if __name__ == "__main__":
    print("=== 方式3：使用进程池 ===")
    print("进程池会自动管理进程的创建和销毁，更高效")
    print("适合处理大量相似的任务")
    
    # 创建进程池，最大工作进程数为3
    # with语句确保进程池在使用后正确关闭
    with multiprocessing.Pool(processes=3) as pool:
        # 提交5个任务到进程池
        
        # 方法1：使用map（最简单）
        print("使用map提交5个任务...")
        # map会阻塞直到所有任务完成
        results = pool.map(pool_worker, range(5))
        print("所有任务结果:", results)
        
        # 方法2：使用apply_async（更灵活）
        print("\n使用apply_async提交3个任务...")
        futures = []  # 保存AsyncResult对象的列表
        
        for i in range(3):
            # 异步提交任务，立即返回AsyncResult对象
            future = pool.apply_async(pool_worker, (i + 10,))
            futures.append(future)
        
        print("任务已提交，正在获取结果...")
        # 获取所有异步任务的结果
        for future in futures:
            # get()方法会阻塞，直到任务完成并返回结果
            result = future.get()
            print(f"收到异步结果: {result}")
    
    print("进程池任务全部完成\n")
```

### 3.2 进程间通信

```python
import multiprocessing
import time
import random

# ========== 队列（Queue） ==========
print("=== 进程间通信：使用队列 ===")

def producer_process(queue, producer_name, item_count):
    """生产者进程函数
    queue: 进程安全队列
    producer_name: 生产者名称
    item_count: 生产项目数量
    """
    current_pid = multiprocessing.current_process().pid
    print(f"生产者 {producer_name} (PID: {current_pid}) 开始工作")
    
    for i in range(item_count):
        # 生产项目
        item = f"{producer_name}-产品-{i+1}"
        # 放入队列
        queue.put(item)
        print(f"生产者 {producer_name} 生产: {item} (队列大小: {queue.qsize()})")
        time.sleep(random.uniform(0.1, 0.3))
    
    # 发送结束信号
    queue.put(None)
    print(f"生产者 {producer_name} 完成工作")

def consumer_process(queue, consumer_name):
    """消费者进程函数
    queue: 进程安全队列
    consumer_name: 消费者名称
    """
    current_pid = multiprocessing.current_process().pid
    print(f"消费者 {consumer_name} (PID: {current_pid}) 开始工作")
    
    item_count = 0
    while True:
        # 从队列获取项目
        item = queue.get()
        
        # 检查结束信号
        if item is None:
            # 放回结束信号，让其他消费者也能结束
            queue.put(None)
            print(f"消费者 {consumer_name} 收到结束信号，停止工作。共消费 {item_count} 个项目")
            break
        
        # 消费项目
        print(f"消费者 {consumer_name} 消费: {item} (队列大小: {queue.qsize()})")
        time.sleep(random.uniform(0.2, 0.4))
        item_count += 1

if __name__ == "__main__":
    print("创建进程安全队列...")
    # 创建进程安全队列（multiprocessing.Queue）
    # 与threading.Queue不同，这个队列可以在进程间安全使用
    queue = multiprocessing.Queue(5)
    
    print("创建生产者和消费者进程...")
    # 创建进程
    producers = [
        multiprocessing.Process(
            target=producer_process, 
            args=(queue, "工厂A", 3)
        ),
        multiprocessing.Process(
            target=producer_process,
            args=(queue, "工厂B", 3)
        )
    ]
    
    consumers = [
        multiprocessing.Process(
            target=consumer_process,
            args=(queue, "消费者1")
        ),
        multiprocessing.Process(
            target=consumer_process,
            args=(queue, "消费者2")
        )
    ]
    
    print("启动消费者进程...")
    # 先启动消费者
    for c in consumers:
        c.start()
    
    print("启动生产者进程...")
    # 再启动生产者
    for p in producers:
        p.start()
    
    print("等待生产者完成...")
    # 等待生产者完成
    for p in producers:
        p.join()
    
    print("等待消费者完成...")
    # 等待消费者完成
    for c in consumers:
        c.join()
    
    print("进程间队列通信演示完成\n")

# ========== 管道（Pipe） ==========
print("=== 进程间通信：使用管道 ===")

def pipe_worker(conn, worker_name):
    """管道工作进程函数
    conn: 管道连接对象
    worker_name: 工作进程名称
    """
    current_pid = multiprocessing.current_process().pid
    print(f"管道工作进程 {worker_name} (PID: {current_pid}) 开始")
    
    # 从管道接收消息
    while True:
        try:
            # 接收消息（阻塞，直到有消息）
            message = conn.recv()
            print(f"工作进程 {worker_name} 收到: {message}")
            
            # 检查结束信号
            if message == "END":
                print(f"工作进程 {worker_name} 收到结束信号")
                break
            
            # 处理消息并回复
            response = f"{worker_name} 处理了: {message}"
            conn.send(response)
            time.sleep(0.5)
            
        except EOFError:
            # 管道已关闭
            print(f"工作进程 {worker_name} 检测到管道关闭")
            break
    
    print(f"管道工作进程 {worker_name} 结束")

if __name__ == "__main__":
    print("创建管道...")
    # 创建管道，返回两个连接对象
    # parent_conn: 父进程使用的连接
    # child_conn: 子进程使用的连接
    parent_conn, child_conn = multiprocessing.Pipe()
    
    print("创建管道工作进程...")
    # 创建工作进程，传递子连接
    worker = multiprocessing.Process(
        target=pipe_worker,
        args=(child_conn, "Worker1")
    )
    worker.start()
    
    # 主进程发送消息给工作进程
    messages = ["消息1", "消息2", "消息3", "END"]
    
    for msg in messages:
        print(f"主进程发送: {msg}")
        parent_conn.send(msg)  # 发送消息
        
        if msg != "END":
            # 接收工作进程的回复
            response = parent_conn.recv()
            print(f"主进程收到回复: {response}")
    
    # 等待工作进程结束
    worker.join()
    print("管道通信演示完成\n")

# ========== 共享内存 ==========
print("=== 进程间通信：使用共享内存 ===")

def shared_memory_worker(shared_value, shared_array, lock, worker_name):
    """共享内存工作进程函数
    shared_value: 共享值
    shared_array: 共享数组
    lock: 进程锁
    worker_name: 工作进程名称
    """
    current_pid = multiprocessing.current_process().pid
    
    # 使用锁保护共享数据的修改
    with lock:
        # 修改共享值
        shared_value.value += 1
        print(f"进程 {worker_name} 修改共享值: {shared_value.value}")
        
        # 修改共享数组
        for i in range(len(shared_array)):
            shared_array[i] = shared_array[i] * 2 + i
        print(f"进程 {worker_name} 修改共享数组: {list(shared_array)}")
    
    time.sleep(1)

if __name__ == "__main__":
    print("创建共享内存和锁...")
    
    # 创建共享值（整型）
    # 'i' 表示整数类型
    shared_value = multiprocessing.Value('i', 0)
    
    # 创建共享数组（整型数组，长度为5）
    # 'i' 表示整数类型，5是数组长度
    shared_array = multiprocessing.Array('i', 5)
    
    # 创建进程锁，用于保护共享数据
    lock = multiprocessing.Lock()
    
    print(f"初始共享值: {shared_value.value}")
    print(f"初始共享数组: {list(shared_array)}")
    
    print("创建多个工作进程修改共享数据...")
    processes = []
    
    for i in range(3):
        p = multiprocessing.Process(
            target=shared_memory_worker,
            args=(shared_value, shared_array, lock, f"Worker-{i+1}")
        )
        processes.append(p)
        p.start()
    
    # 等待所有进程完成
    for p in processes:
        p.join()
    
    print(f"最终共享值: {shared_value.value}")
    print(f"最终共享数组: {list(shared_array)}")
    print("共享内存演示完成\n")
```

## 4. 协程（Coroutine）

### 4.1 协程基础

```python
import asyncio  # Python的异步IO模块
import time
import random

# ========== 异步函数基础 ==========
print("=== 协程基础：理解async/await ===")

async def simple_coroutine(name, delay):
    """简单的协程函数
    name: 协程名称
    delay: 延迟时间（秒）
    """
    print(f"协程 {name} 开始执行 - 时间: {time.strftime('%H:%M:%S')}")
    
    # 使用await等待异步操作完成
    # asyncio.sleep() 是异步的，不会阻塞事件循环
    await asyncio.sleep(delay)
    
    print(f"协程 {name} 执行完成 - 时间: {time.strftime('%H:%M:%S')}")
    return f"{name}-结果"

async def main_basic():
    """主协程函数 - 演示基本用法"""
    print("=== 基本协程演示 ===")
    
    # 创建协程对象（不会立即执行）
    coro1 = simple_coroutine("协程1", 2)
    coro2 = simple_coroutine("协程2", 3)
    
    print("开始执行协程...")
    
    # 方法1：依次等待（类似同步）
    print("方法1：依次执行（类似同步）")
    start_time = time.time()
    
    result1 = await coro1  # 等待协程1完成
    print(f"收到结果1: {result1}")
    
    result2 = await coro2  # 等待协程2完成
    print(f"收到结果2: {result2}")
    
    end_time = time.time()
    print(f"总执行时间: {end_time - start_time:.2f}秒")
    
    # 方法2：并发执行（真正体现协程优势）
    print("\n方法2：并发执行")
    start_time = time.time()
    
    # 使用asyncio.gather并发执行多个协程
    results = await asyncio.gather(
        simple_coroutine("并发协程1", 2),
        simple_coroutine("并发协程2", 3),
        simple_coroutine("并发协程3", 1)
    )
    
    end_time = time.time()
    print(f"并发执行结果: {results}")
    print(f"总执行时间: {end_time - start_time:.2f}秒 (注意：是最大延迟时间，不是总和)")

# 运行基本演示
print("运行基本协程演示...")
asyncio.run(main_basic())
print()

# ========== 协程与线程对比 ==========
print("=== 协程 vs 线程：性能对比 ===")

def io_operation_blocking(operation_id):
    """模拟阻塞式IO操作（线程版本）"""
    print(f"阻塞IO操作 {operation_id} 开始")
    time.sleep(1)  # 模拟IO等待（阻塞）
    print(f"阻塞IO操作 {operation_id} 完成")
    return f"blocking-{operation_id}"

async def io_operation_async(operation_id):
    """模拟异步IO操作（协程版本）"""
    print(f"异步IO操作 {operation_id} 开始")
    await asyncio.sleep(1)  # 模拟IO等待（非阻塞）
    print(f"异步IO操作 {operation_id} 完成")
    return f"async-{operation_id}"

async def compare_performance():
    """对比协程和线程的性能"""
    print("创建100个IO任务进行对比...")
    
    # 协程版本
    print("协程版本开始...")
    start_time = time.time()
    
    # 创建100个异步任务
    async_tasks = [io_operation_async(i) for i in range(10)]  # 减少到10个以便观察
    # 并发执行所有任务
    async_results = await asyncio.gather(*async_tasks)
    
    async_time = time.time() - start_time
    print(f"协程版本完成，时间: {async_time:.2f}秒")
    
    # 线程版本
    print("线程版本开始...")
    start_time = time.time()
    
    import concurrent.futures
    # 使用线程池
    with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:
        # 提交任务
        futures = [executor.submit(io_operation_blocking, i) for i in range(10)]
        # 获取结果
        thread_results = [future.result() for future in futures]
    
    thread_time = time.time() - start_time
    print(f"线程版本完成，时间: {thread_time:.2f}秒")
    
    print(f"\n性能对比:")
    print(f"协程: {async_time:.2f}秒")
    print(f"线程: {thread_time:.2f}秒")
    print(f"协程比线程快: {thread_time/async_time:.1f}倍 (在大量IO任务时更明显)")

# 运行性能对比
print("运行协程与线程性能对比...")
asyncio.run(compare_performance())
print()

# ========== 协程高级特性 ==========
print("=== 协程高级特性 ===")

async def producer_coroutine(queue, name, count):
    """生产者协程"""
    for i in range(count):
        item = f"{name}-item-{i}"
        await queue.put(item)  # 异步放入队列
        print(f"生产者 {name} 生产: {item}")
        await asyncio.sleep(random.uniform(0.1, 0.3))
    
    # 发送结束信号
    await queue.put(None)

async def consumer_coroutine(queue, name):
    """消费者协程"""
    item_count = 0
    while True:
        item = await queue.get()  # 异步获取队列项目
        
        if item is None:
            # 结束信号，放回以便其他消费者也能结束
            await queue.put(None)
            print(f"消费者 {name} 结束，共消费 {item_count} 个项目")
            break
        
        print(f"消费者 {name} 消费: {item}")
        await asyncio.sleep(random.uniform(0.2, 0.4))
        item_count += 1
        queue.task_done()

async def advanced_features():
    """演示协程高级特性"""
    print("=== 协程队列和任务管理 ===")
    
    # 创建异步队列
    queue = asyncio.Queue(maxsize=5)
    
    # 创建生产者和消费者任务
    producer_tasks = [
        asyncio.create_task(producer_coroutine(queue, "ProducerA", 3)),
        asyncio.create_task(producer_coroutine(queue, "ProducerB", 3))
    ]
    
    consumer_tasks = [
        asyncio.create_task(consumer_coroutine(queue, "Consumer1")),
        asyncio.create_task(consumer_coroutine(queue, "Consumer2"))
    ]
    
    print("启动所有协程任务...")
    
    # 等待生产者完成
    await asyncio.gather(*producer_tasks)
    print("所有生产者完成")
    
    # 等待队列清空
    await queue.join()
    print("队列已清空")
    
    # 发送结束信号
    await queue.put(None)
    
    # 等待消费者完成
    await asyncio.gather(*consumer_tasks)
    print("所有消费者完成")
    
    # 演示超时控制
    print("\n=== 协程超时控制 ===")
    
    async def long_running_task():
        """长时间运行的任务"""
        await asyncio.sleep(5)
        return "任务完成"
    
    try:
        # 设置3秒超时
        result = await asyncio.wait_for(long_running_task(), timeout=3.0)
        print(f"任务结果: {result}")
    except asyncio.TimeoutError:
        print("任务超时！")
    
    # 演示协程取消
    print("\n=== 协程取消 ===")
    
    async def cancellable_task():
        """可取消的任务"""
        try:
            print("可取消任务开始")
            await asyncio.sleep(10)
            print("可取消任务完成")
            return "任务成功"
        except asyncio.CancelledError:
            print("可取消任务被取消")
            raise
    
    task = asyncio.create_task(cancellable_task())
    await asyncio.sleep(1)  # 让任务运行一会儿
    task.cancel()  # 取消任务
    
    try:
        await task
    except asyncio.CancelledError:
        print("任务已确认取消")

# 运行高级特性演示
print("运行协程高级特性演示...")
asyncio.run(advanced_features())
print("\n协程部分演示完成")
```

## 5. 实战应用与总结

### 5.1 综合实战：Web爬虫对比

```python
import requests
import asyncio
import aiohttp
import threading
import multiprocessing
import time
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor

# ========== 同步版本（基准） ==========
def sync_download(url):
    """同步下载函数"""
    try:
        response = requests.get(url, timeout=10)
        return f"下载成功: {url} - 状态码: {response.status_code} - 长度: {len(response.text)}"
    except Exception as e:
        return f"下载失败: {url} - 错误: {e}"

def sync_crawler(urls):
    """同步爬虫"""
    print("=== 同步爬虫开始 ===")
    start_time = time.time()
    
    results = []
    for url in urls:
        result = sync_download(url)
        results.append(result)
        print(result)
    
    end_time = time.time()
    print(f"同步爬虫完成，耗时: {end_time - start_time:.2f}秒")
    return results

# ========== 多线程版本 ==========
def thread_download(url):
    """线程下载函数"""
    return sync_download(url)

def thread_crawler(urls):
    """多线程爬虫"""
    print("=== 多线程爬虫开始 ===")
    start_time = time.time()
    
    with ThreadPoolExecutor(max_workers=10) as executor:
        futures = [executor.submit(thread_download, url) for url in urls]
        results = [future.result() for future in futures]
    
    for result in results:
        print(result)
    
    end_time = time.time()
    print(f"多线程爬虫完成，耗时: {end_time - start_time:.2f}秒")
    return results

# ========== 多进程版本 ==========
def process_download(url):
    """进程下载函数"""
    return sync_download(url)

def process_crawler(urls):
    """多进程爬虫"""
    print("=== 多进程爬虫开始 ===")
    start_time = time.time()
    
    with ProcessPoolExecutor(max_workers=5) as executor:
        futures = [executor.submit(process_download, url) for url in urls]
        results = [future.result() for future in futures]
    
    for result in results:
        print(result)
    
    end_time = time.time()
    print(f"多进程爬虫完成，耗时: {end_time - start_time:.2f}秒")
    return results

# ========== 协程版本 ==========
async def async_download(session, url):
    """异步下载函数"""
    try:
        async with session.get(url, timeout=10) as response:
            text = await response.text()
            return f"下载成功: {url} - 状态码: {response.status} - 长度: {len(text)}"
    except Exception as e:
        return f"下载失败: {url} - 错误: {e}"

async def async_crawler(urls):
    """异步爬虫"""
    print("=== 异步爬虫开始 ===")
    start_time = time.time()
    
    async with aiohttp.ClientSession() as session:
        tasks = [async_download(session, url) for url in urls]
        results = await asyncio.gather(*tasks)
    
    for result in results:
        print(result)
    
    end_time = time.time()
    print(f"异步爬虫完成，耗时: {end_time - start_time:.2f}秒")
    return results

# ========== 性能对比 ==========
async def compare_crawlers():
    """对比不同并发模型的爬虫性能"""
    # 测试URL列表（使用一些可公开访问的API）
    test_urls = [
        "https://httpbin.org/delay/1",  # 延迟1秒的接口
        "https://httpbin.org/delay/1",
        "https://httpbin.org/delay/1",
        "https://httpbin.org/delay/1",
        "https://httpbin.org/delay/1",
        "https://httpbin.org/delay/2",  # 延迟2秒的接口
        "https://httpbin.org/delay/2",
        "https://httpbin.org/delay/2",
    ]
    
    print(f"测试URL数量: {len(test_urls)}")
    print("开始性能对比...\n")
    
    # 同步版本
    sync_start = time.time()
    sync_results = sync_crawler(test_urls)
    sync_time = time.time() - sync_start
    
    print("\n" + "="*50 + "\n")
    
    # 多线程版本
    thread_start = time.time()
    thread_results = thread_crawler(test_urls)
    thread_time = time.time() - thread_start
    
    print("\n" + "="*50 + "\n")
    
    # 多进程版本
    process_start = time.time()
    process_results = process_crawler(test_urls)
    process_time = time.time() - process_start
    
    print("\n" + "="*50 + "\n")
    
    # 协程版本
    async_start = time.time()
    async_results = await async_crawler(test_urls)
    async_time = time.time() - async_start
    
    print("\n" + "="*50)
    print("=== 性能对比总结 ===")
    print(f"同步版本:   {sync_time:.2f}秒")
    print(f"多线程版本: {thread_time:.2f}秒 - 比同步快 {sync_time/thread_time:.1f}倍")
    print(f"多进程版本: {process_time:.2f}秒 - 比同步快 {sync_time/process_time:.1f}倍")
    print(f"协程版本:   {async_time:.2f}秒 - 比同步快 {sync_time/async_time:.1f}倍")
    
    # 验证结果一致性
    assert len(sync_results) == len(thread_results) == len(process_results) == len(async_results)
    print("所有版本结果数量一致，测试完成")

# 运行爬虫对比
print("开始Web爬虫性能对比演示...")
asyncio.run(compare_crawlers())
```

### 5.2 选择指南与最佳实践

```python
"""
Python并发编程选择指南：

1. 选择依据：
   - CPU密集型任务（计算、数据处理）：多进程
   - IO密集型任务（网络、文件）：多线程或协程
   - 高并发IO（大量网络请求）：协程
   - 简单并行任务：线程池/进程池

2. 性能考虑：
   - 协程 > 多线程 > 多进程（在IO密集型任务中）
   - 多进程 > 多线程 > 协程（在CPU密集型任务中，由于GIL）

3. 复杂度考虑：
   - 协程：中等（需要理解async/await）
   - 多线程：中等（需要注意线程安全）
   - 多进程：较高（进程间通信复杂）

4. 内存使用：
   - 协程：很低（轻量级）
   - 多线程：较低（共享内存）
   - 多进程：较高（每个进程独立内存）

最佳实践：

1. 线程安全：
   - 使用锁保护共享数据
   - 使用线程安全的数据结构（Queue）
   - 避免全局变量

2. 资源管理：
   - 使用with语句管理资源
   - 使用线程池/进程池避免频繁创建销毁
   - 及时释放锁和信号量

3. 错误处理：
   - 协程中使用try-except处理异常
   - 线程中使用Queue的task_done和join
   - 进程中使用进程安全的数据结构

4. 调试技巧：
   - 使用logging记录线程/进程ID
   - 设置适当的超时时间
   - 使用调试工具（如pdb）

记住：没有银弹，根据具体场景选择最合适的并发模型！
"""

# 简单决策流程图
def select_concurrency_model(task_type, concurrency_level, complexity_tolerance):
    """
    选择并发模型的简单决策函数
    
    Parameters:
    task_type: 'cpu'（计算密集型）或 'io'（IO密集型）
    concurrency_level: 'low'（低并发）、'medium'（中等）、'high'（高并发）
    complexity_tolerance: 'low'（低复杂度）、'high'（高复杂度）
    """
    if task_type == 'cpu':
        return "多进程（绕过GIL，利用多核）"
    elif task_type == 'io':
        if concurrency_level == 'high':
            if complexity_tolerance == 'high':
                return "协程（最高性能）"
            else:
                return "多线程（平衡性能和复杂度）"
        else:
            return "多线程（简单有效）"
    else:
        return "请选择正确的任务类型"

# 使用示例
print("=== 并发模型选择指南 ===")
print("场景1：数据分析（CPU密集型，中等并发，能接受复杂度）")
print("推荐:", select_concurrency_model('cpu', 'medium', 'high'))
print()

print("场景2：Web爬虫（IO密集型，高并发，希望高性能）")
print("推荐:", select_concurrency_model('io', 'high', 'high'))
print()

print("场景3：文件处理（IO密集型，低并发，希望简单）")
print("推荐:", select_concurrency_model('io', 'low', 'low'))
print()

print("场景4：实时数据处理（IO密集型，高并发，能接受复杂度）")
print("推荐:", select_concurrency_model('io', 'high', 'high'))
```

## 总结

这份超详细的并发编程笔记涵盖了：

### 核心概念：
- **进程**：独立内存空间，适合CPU密集型任务
- **线程**：共享内存空间，适合IO密集型任务  
- **协程**：用户态轻量级线程，适合高并发IO任务
- **GIL**：Python线程的限制，了解其对性能的影响

### 关键技术：
- **线程同步**：锁、可重入锁、条件变量、信号量、事件
- **线程通信**：队列、管道、共享内存
- **资源管理**：线程池、进程池、上下文管理器
- **异步编程**：async/await、任务管理、超时控制

### 实战应用：
- Web爬虫性能对比
- 生产者消费者模式
- 资源池管理
- 错误处理和调试

### 选择指南：
根据任务类型（CPU/IO密集型）、并发级别和复杂度容忍度选择最合适的并发模型。

记住：**理解原理比记住API更重要**！在实际项目中，先从简单的开始，逐步优化到最适合的并发方案。